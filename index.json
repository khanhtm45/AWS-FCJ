[{"uri":"https://khanhtm45.github.io/AWS-FCJ/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AI-powered planning, design, and coding for modern software development” Overview: An on-demand webinar from AWS Marketplace focused on bringing Generative AI into planning – design – coding across the SDLC to accelerate collaboration, automate testing, generate docs/diagrams, and enforce security via a zero-trust approach. (Source: AWS Marketplace webinar page – On-demand).\nEvent Objectives Explain how GenAI reshapes planning, design, and coding on AWS. Show how to integrate GenAI into Agile/Scrum: sprint planning, backlog refinement, test generation. Demonstrate creating UI/UX mock-ups, architecture diagrams, and technical docs with AI for faster alignment. Share security/zero-trust practices for code analysis and architecture reviews. Speakers Harrison Kirby — Ambassador, DevOps Institute Ronak Shah — Principal Solutions Architect, AWS Highlights 1) GenAI across the SDLC Planning \u0026amp; Design: AI proposes architecture options and generates diagrams/docs; supports early decision-making. Coding \u0026amp; Testing: Real-time code suggestions, unit/integration test generation, fewer fix-rework cycles. Collaboration: Rapid UI/UX mock-ups to align architects, developers, and designers. 2) “Attendees will learn” (from the webinar page) Embedding GenAI into Agile workflows for sprint planning, backlog refinement, test generation. Using AI-generated visuals/diagrams/code to accelerate cross-functional collaboration. Applying security frameworks and zero-trust principles with AI for architecture reviews and code analysis. 3) Practical tie-ins (from your supporting document) Governance Copilot: flags scope creep/budget drift; auto-creates minutes and risk registers. Smarter Estimation: learns from historical projects; outputs best/worst-case ranges, not a single point. Scope Clarifier: NLP capture/analysis to detect ambiguous/conflicting/missing requirements before scope lock. Dependency Radar: AI-graph mapping of team/vendor/module dependencies to avoid bottlenecks. Auto-documentation: keeps UML/sequence/workflow/API docs in sync with code/design changes. Key Takeaways Vision → Value: anchor every GenAI effort to clear KPIs/ROI (speed, cost, quality). Data-first: retrieval/embedding/rerank quality drives output quality. Security-by-design: zero-trust, access control, PII protection, content moderation, cost/token visibility. Observability \u0026amp; Eval: tracing, online/offline evaluation, continuous feedback loops. Applying to Work Pilot 1–2 GenAI use cases over 6–8 weeks with go/no-go gates (quality, latency, cost/interaction, adoption). Enable a governance copilot (scope/budget alerts) and auto-documentation from the first sprint. Standardize estimations with historical data; publish 2–3 scenarios instead of one number. Use a scope clarifier for all requirement sessions; run a dependency radar before major design milestones. Event Experience The webinar shows how to operationalize AI—from documentation/architecture to code/test and security—helping teams reduce process friction and shorten lead time while maintaining safety and scalability on AWS.\nSome event photos Add your screenshots/photos here\nIn short, the session outlines measurable GenAI steps across the SDLC: AI does the heavy lifting, while humans supervise and decide.\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Day Vietnam 2025 – AI Edition” Theme \u0026amp; Tagline: New Age Vietnam: From Vision to Value — turning AI vision into measurable value.\nFocus: Generative AI with complementary Cloud capabilities (data, security, infrastructure, modern apps).\nEvent Objectives Drive Vision → Value: tie GenAI to clear KPIs/ROI (speed, cost, quality). Present GenAI reference architectures (RAG, agentic workflows, data pipelines, guardrails). Define a practical adoption path: Idea → PoC → Pilot → Scale with governance \u0026amp; data safety. Equip builders/engineers and IT leaders with AWS best practices (observability, cost control). Speakers AWS Vietnam leaders \u0026amp; specialists (AI/GenAI, Data, Security, AppMod). Customer speakers from Vietnam (transformation \u0026amp; GenAI use cases). Partners (deployment playbooks, cost optimization, operations). Key Highlights Central theme: Generative AI on AWS Amazon Bedrock / Amazon Q \u0026amp; Guardrails: rapid start, security, compliance. Data-first: retrieval/rerank/embeddings quality drives output quality. Cloud consistency: build on AWS foundations (identity, networking, storage, monitoring) for security \u0026amp; scale. Representative tracks GenAI Foundations \u0026amp; Architecture: RAG, tool-use/agents, evaluation \u0026amp; observability. Data, Security \u0026amp; Governance: PII protection, access control, moderation, cost/token tracking. App Modernization for AI: service decomposition, API-first, event-driven; making apps AI-ready. Builders Hands-on: demos/mini-labs for internal assistants and semantic search. Key messages Vision → Value: avoid demo-only efforts; anchor to KPIs from day one. Data before models: optimize pipelines, indexing, caching, and latency. Safety \u0026amp; compliance: guardrails, auditability, continuous monitoring \u0026amp; evaluation. Key Takeaways Design Mindset Business-first: small, high-impact problems with clear owners \u0026amp; data. Ubiquitous language: align business–tech and lock KPIs/ROI early. Technical Architecture Prefer RAG and agentic patterns; separate orchestrator from tools. Observability \u0026amp; Eval: tracing, offline/online evaluation, feedback loops. Cost \u0026amp; Performance: fit-for-purpose models, batching/caching, prompt/context optimization, latency control. Modernization Strategy Roadmap PoC → pilot → limited rollout → scale with exit criteria per stage. Build on AWS foundations for security, resilience, and scalability. Applying to Work Shortlist 1–2 GenAI use cases tied to clear KPIs; prepare data \u0026amp; access. Run a 6–8 week PoC with go/no-go gates (quality, latency, cost/interaction, adoption). Ops plan: monitoring, guardrails, human-in-the-loop, continuous improvement. Event Experience The event offered a clear path to move from vision to value: deploying GenAI into real workflows with security, cost awareness, and business alignment. Talks and demos showed how to standardize the data pipeline, implement RAG/agents, and measure impact to unlock pilot \u0026amp; scale.\nSome event photos Add your event photos here\nIn short, the “AI Edition” underscores that Generative AI creates real value when paired with the right data, architecture, governance, and KPIs on AWS.\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Trương Minh Khánh\nPhone Number: 8948333703\nEmail: khanhtmse182131@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/11/2025\nReport Contents Worklog Proposal Translated Blogs Participated Events Workshop Self-evaluation Feedback and Comments "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"Workshop Overview In this workshop, you will build a complete AI Chatbot using serverless architecture on AWS. The chatbot uses Claude Haiku 4.5 deployed entirely serverless with low costs and automatic scaling capabilities.\nModules Module 1: Setup Amazon Bedrock Module 2: Create Lambda Function Module 3: Configure API Gateway Module 4: Deploy Frontend to S3 Module 5: Testing \u0026amp; Debugging\nArchitecture Overview AWS Services \u0026amp; Their Roles in the Workshop 1. Amazon Bedrock — AI Brain Process and generate responses from user messages. Uses Claude Haiku 4.5 (Messages API). Features: Inference Profiles Conversation history management Temperature control 2. AWS Lambda — Backend Logic Role:\nReceive requests from API Gateway Validate input, format prompt Call Bedrock and return response Features:\nNode.js 24 runtime Environment variables (MODEL_ID, ALLOWED_ORIGIN) CloudWatch Logs Input validation + error handling Retry logic CORS handling 3. Amazon API Gateway — API Endpoint Role:\nPublic REST API for browsers CORS management Route POST /chat to Lambda Throttling to prevent abuse Features:\nREST API or HTTP API CORS configuration Lambda Proxy Integration OPTIONS method for CORS preflight 4. Amazon S3 — Frontend Hosting Static Website Hosting Host HTML/CSS/JS Allow users to access chatbot via browser 5. Amazon CloudWatch — Monitoring \u0026amp; Debugging Collect logs from Lambda Debug request/response \u0026amp; errors Workshop Learning Objectives After the workshop, you will:\nUnderstand serverless architecture and how services interact Deploy a production-ready chatbot with Claude Haiku 4.5 Integrate Bedrock API with Lambda Configure API Gateway Host static website on S3 Debug using CloudWatch Logs Optimize serverless costs "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.3-setup-amazon-bedrock/5.3.1-check-model/","title":"Setup Amazon Bedrock","tags":[],"description":"","content":"1. Go to Amazon Bedrock 2. Click on Model Catalog to find Claude Haiku 4.5 3. Click on Claude Haiku 4.5 and scroll down to find the Model ID 4. You can click \u0026ldquo;open in playground\u0026rdquo; to test the model Note: Currently you no longer need to go to model access to request model access. You can directly use the API of a model in Amazon Bedrock with an IAM account that has AWS marketplace permission once you call the model once to activate it References: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/4-eventparticipated/4.3-event3/","title":"Event 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AI-Driven Development Lifecycle: Reshaping Software Engineering\u0026rdquo; Event Information Date: Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nOrganizer: AWS Idolize Team\nParticipants: Over 300 registered attendees\nFormat: In-person Event at Bitexco Tower\nEvent Agenda 14:00 - 14:15: Reception 14:15 - 15:30: Overview of AI Development Lifecycle and Amazon Q Developer Demo (by Toàn Huỳnh) 15:30 - 15:45: Break 15:45 - 16:30: Kiro Demonstration (by My Nguyen) Event Context The rise of generative AI marks a revolutionary shift in software development. Generative AI is reshaping how developers and organizations learn, plan, create, deploy, and securely manage applications.\nBy integrating AI into the software development lifecycle - from architecture to development, testing, deployment, and maintenance, developers can automate undifferentiated heavy-lifting tasks. This automation boosts productivity and enables developers to focus on more creative, high-value tasks.\nEvent Objectives Explore AI-Driven Development using Amazon Q Developer and Kiro Introduce AI-powered Software Development Lifecycle (AIDOC) methodology Demonstrate practical applications of AI in software development Showcase Amazon Q Developer capabilities Present Kiro tool for enhanced developer productivity Share best practices for integrating AI into development workflows Speakers Toàn Huỳnh – Senior Solutions Architect, AWS Mỹ Nguyễn – Senior Prototyping Engineer, AWS Coordinators Diễm My – AWS Team Đại Trường – AWS Team Đinh Nguyên – AWS Team Key Highlights The Need for AI in Software Development Many developers are already using AI tools to increase productivity, but challenges remain:\nSimple tasks work well, but complex projects face issues Lack of methodology for integrating AI across the full development lifecycle Quality control concerns when AI generates large amounts of code Context management issues with token limits in large projects AI Development Lifecycle (AIDOC) Methodology AIDOC is not just a tool—it\u0026rsquo;s a comprehensive methodology that helps developers work effectively with AI throughout the software development process.\nCore Principles:\nHuman-AI Collaboration: AI assists, but humans validate, make decisions, and oversee Multi-step Problem Solving: Break down complex tasks instead of single-shot prompts Validation at Every Stage: Review AI outputs before proceeding to the next step Developer Ownership: Developers remain responsible for all code quality Three Levels of AI Integration AI Assistant (2023): Line-by-line code suggestions and auto-completion AI Assistant+ (2024): Solves larger tasks, provides solution options AI Agents (2025): Autonomous reasoning, planning, and multi-file generation Key Challenges When Using AI Single-shot limitations: Generic prompts produce generic results Token overflow: Context limits when working on large projects Existing codebase integration: Difficulty implementing features in established projects Quality assurance: Ensuring AI-generated code meets standards Decision making: Determining what AI should handle vs. human judgment Context management: Providing relevant information to AI Validation complexity: Reviewing and approving AI outputs AIDOC Workflow Instead of asking AI to complete entire features at once, AIDOC follows a structured approach:\nUnderstanding: AI helps analyze user stories and requirements Planning: AI generates implementation plans for developer review Design: Create logical design and identify affected layers Implementation: AI implements approved plans step-by-step Testing: Validate outputs at each stage Review: Human oversight ensures quality and correctness Amazon Q Developer Demonstration Toàn Huỳnh demonstrated how Amazon Q Developer integrates into the development workflow:\nIntelligent code completion with context awareness Multi-file code generation for complex features Architectural guidance for solution design Security scanning and best practice recommendations Natural language interaction for problem-solving Developer Responsibilities in AI-Powered Development Validation: Verify AI-generated code and designs Decision Making: Choose between AI-suggested options Oversight: Maintain control over the development process Quality Assurance: Ensure code meets standards Authorship: Take ownership of all delivered code Key Takeaways AI won\u0026rsquo;t replace developers, but developers who use AI effectively will outperform those who don\u0026rsquo;t Methodology matters: Having a structured approach (like AIDOC) is crucial for success Human oversight is essential: Developers must validate, decide, and oversee all AI outputs Productivity gains are real: Developers can increase output from 50 to 70+ user stories per month Multi-step approach works best: Break complex tasks into manageable steps Context is critical: Provide AI with relevant information for better results Quality remains human responsibility: Developers are the authors of their code Personal Insights This workshop highlighted how AI is transforming software development from a tool perspective to a methodology shift. The emphasis on human-AI collaboration rather than replacement was particularly important. The AIDOC framework provides a practical approach to integrating AI while maintaining quality and developer control.\nThe demonstrations of Amazon Q Developer and Kiro showed that AI tools have matured significantly, moving from simple code completion to complex multi-file generation and architectural guidance. However, the key message remains: developers must understand, validate, and own their code regardless of how it\u0026rsquo;s generated.\nUpcoming AWS Events Gen AI Game Day (Next month): Hands-on competition using Gen AI tools December Competition: Showcase Gen AI products (10-15 minute presentations) Applying to Work Adopt AIDOC methodology: Structure AI interactions in multi-step workflows Use Amazon Q Developer: Integrate into daily development for productivity boost Implement validation checkpoints: Review AI outputs at each development stage Break down complex tasks: Divide large features into manageable AI-assisted steps Maintain code ownership: Ensure understanding and quality of all AI-generated code Experiment with AI agents: Explore autonomous code generation for appropriate use cases Event Experience Attending the \u0026ldquo;AI Software Development Lifecycle\u0026rdquo; workshop was extremely valuable, providing comprehensive insights into modern AI-powered development practices. Key experiences included:\nLearning from Industry Experts AWS senior engineers shared real-world experiences using AI in enterprise projects Practical demonstrations showed both capabilities and limitations of current AI tools Understanding the evolution from simple AI assistants to autonomous AI agents Hands-on Technical Knowledge Witnessed live demonstrations of Amazon Q Developer solving complex coding challenges Learned the AIDOC methodology for structuring AI-assisted development Understood how to manage context and token limits in large projects Discovered techniques for breaking down user stories for AI implementation Mindset Transformation Realized AI is a collaborator, not a replacement for developers Understood the importance of validation and ownership in AI-generated code Learned to think in terms of multi-step problem solving rather than single prompts Recognized the value of methodology over tools alone Networking and Community Connected with 300+ developers interested in AI-powered development Exchanged ideas about AI integration challenges and solutions Learned about upcoming AWS community events and competitions Practical Tools and Techniques Amazon Q Developer for intelligent code completion and generation Kiro tool for enhanced productivity Multi-step prompting strategies for better AI responses Context management techniques for large codebases Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also reshaped my understanding of how AI can enhance the software development lifecycle. The emphasis on human oversight and validation ensures that AI serves as a powerful tool to amplify developer capabilities rather than replace them.\nConclusion The AI Software Development Lifecycle represents a fundamental shift in how we approach software development. By combining AI capabilities with human expertise through structured methodologies like AIDOC, developers can achieve significant productivity gains while maintaining quality and control. The key is understanding that AI is a powerful collaborator that requires proper guidance, validation, and oversight to deliver optimal results.\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/3-blogstranslated/3.1-blog1/","title":"Announcing the Developer Preview of DynamoDB Mapper for Kotlin","tags":[],"description":"","content":"Announcing the Developer Preview of DynamoDB Mapper for Kotlin by Ian Botsford | on 30 OCT 2024\nWe\u0026rsquo;re excited to announce the Developer Preview of DynamoDB Mapper for Kotlin. This high-level library provides streamlined, idiomatic ways for developers to map data between their business logic written in Kotlin and their tables in DynamoDB. DynamoDB Mapper works with most Kotlin data classes right out of the box and also offers powerful features for modeling data flexibly and handling more complex schemas. DynamoDB Mapper is part of the AWS SDK for Kotlin and will be released on the same daily schedule as other SDK components.\nYou can test out this new feature by using the DynamoDB Mapper Schema Generator plugin for Gradle and annotating your data classes:\n@DynamoDbItem data class Planet( @DynamoDbPartitionKey val name: String, val radiusKilometers: Double, val atmosphericGasses: List\u0026lt;String\u0026gt;, val hasRings: Boolean, val numberOfMoons: Int, ) The preceding code sample will automatically generate item schemas at build time. These schemas may then be used to access DynamoDB table items as Planet instances as shown in the following code:\nval client = DynamoDbClient.fromEnvironment() val mapper = DynamoDbMapper(client) val planetsTable = mapper.getTable(\u0026#34;all-planets\u0026#34;, PlanetSchema) val saturn = Planet( name = \u0026#34;Saturn\u0026#34;, radiusKilometers = 60268.0, atmosphericGasses = listOf(\u0026#34;hydrogen\u0026#34;, \u0026#34;helium\u0026#34;, \u0026#34;methane\u0026#34;), hasRings = true, numberOfMoons = 146, ) // Write the `saturn` object to the planets table planetsTable.putItem { item = saturn } // Get a Planet instance from an item with the key \u0026#34;Jupiter\u0026#34; val jupiter = planetsTable.getItem(\u0026#34;Jupiter\u0026#34;).item println(jupiter) // Prints \u0026#34;Planet(name=Jupiter, radiusKilometers=71492.0, ...)\u0026#34; Preview functionality This DynamoDB Mapper is being released as a Developer Preview, meaning it is not yet feature complete, may contain bugs, and is not suitable for production workloads. The goal of this Developer Preview is to gather early feedback from eager adopters, which may lead to redesigns and backwards-incompatible changes. See the AWS SDKs and Tools maintenance policy for more details on product lifecycle.\nThe initial Developer Preview supports the following DynamoDB operations:\ndeleteItem getItem putItem queryPaginated scanPaginated It also supports the following types of DynamoDB expressions:\nFilter expressions Key condition expressions Support for more operations such as updateItem and createTable, along with support for more types of expressions such as update expressions and projection expressions, will be added before DynamoDB Mapper exits Developer Preview.\nNext steps To get started with the DynamoDB Mapper for Kotlin, check out the following links:\nGetting started guide API reference documentation Our GitHub repository Let us know how this new feature works for you and whether it meets your needs. If you are curious about something, post or join a discussion in our GitHub repo. If you\u0026rsquo;ve found a bug, please file an issue in our GitHub repo.\nTags: Amazon DynamoDB, aws-sdk, Kotlin, SDK\nAuthor: Ian Botsford - Ian is a developer working on the AWS SDK for Kotlin. He is passionate about making AWS easy to use through fluent, idiomatic SDKs. You can find him on GitHub at @ianbotsf.\nSource: AWS Developer Tools Blog\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Creating Your First AWS Account + Create new AWS account + Set up MFA for root user + Create Administrator Group and Admin User + Configure AWS Management Console 08/09/2025 08/09/2025 https://000001.awsstudygroup.com/ 3 - Workshop: Cost Management with AWS Budgets + Learn about AWS Budgets types + Create Cost Budget + Create Usage Budget + Create Reservation Budget + Create Savings Plans Budget 09/09/2025 09/09/2025 https://000007.awsstudygroup.com/ 4 - Workshop: Request Support with AWS Support + Learn about AWS Support Plans + Access AWS Support Center + Manage support requests and cases 10/09/2025 10/09/2025 https://000009.awsstudygroup.com/ 5 - Workshop: AWS IAM Access Control + Create IAM Users and Groups + Configure IAM Policies + Create and manage IAM Roles + Practice switching roles 11/09/2025 11/09/2025 https://000002.awsstudygroup.com/ 6 - Workshop: Amazon VPC and Site-to-Site VPN + Introduction to Amazon VPC + Configure Security Groups and Network ACLs + Deploy EC2 instances + Set up AWS Site-to-Site VPN connection 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/ Week 1 Achievements: Day 2 - AWS Account Setup:\nSuccessfully created a new AWS account with proper configuration Enabled Multi-Factor Authentication (MFA) for the root user account to enhance security Created an Administrator IAM Group with appropriate policies Created an Admin User and added to the Administrator Group Configured AWS Management Console and familiarized with the interface Learned best practices for AWS account security and root user management Day 3 - AWS Cost Management:\nUnderstood the importance of AWS Budgets for cost control and monitoring Learned about different budget types: Cost, Usage, Reservation, and Savings Plans Successfully created Cost Budgets to track AWS spending Configured Usage Budgets to monitor specific service consumption Set up Reservation Budgets for Reserved Instances utilization tracking Created Savings Plans Budgets for commitment monitoring Configured budget alerts with multiple thresholds (50%, 80%, 90%, 100%) Learned about budget actions and automated cost control measures Day 4 - AWS Support:\nUnderstood different AWS Support Plans: Basic, Developer, Business, and Enterprise Learned about response times and support features for each plan Successfully accessed AWS Support Center through the Management Console Created and managed support cases Learned how to provide necessary information for effective support requests Understood the support case lifecycle and follow-up procedures Day 5 - IAM Access Control:\nMastered AWS Identity and Access Management (IAM) core concepts Successfully created IAM Users with appropriate permissions Created IAM Groups and applied policies for organized access management Configured IAM Roles for temporary access and cross-account scenarios Practiced switching between IAM roles for different permission levels Implemented the principle of least privilege in access control Understood the difference between IAM Users, Groups, and Roles Learned IAM best practices and security recommendations Day 6 - Amazon VPC and Site-to-Site VPN:\nGained comprehensive understanding of Amazon Virtual Private Cloud (VPC) Learned about VPC components: subnets, route tables, internet gateways Configured Security Groups for instance-level security Set up Network ACLs for subnet-level security Successfully deployed EC2 instances within VPC environment Configured AWS Site-to-Site VPN connection for secure hybrid connectivity Understood VPN routing and encryption protocols Learned about production-ready VPC architectures with multi-AZ setup Implemented VPC Flow Logs for network monitoring Overall Achievements:\nCompleted 5 comprehensive AWS workshops covering foundational services Gained hands-on experience with AWS account management and security Developed skills in cost management and monitoring Mastered IAM access control and security best practices Built practical knowledge of VPC networking and secure connectivity Ready to implement secure, cost-effective AWS solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Introduction to Amazon EC2 + Launch Windows and Linux instances + Learn EC2 fundamentals + Deploy AWS User Management application + Cost \u0026amp; Usage Governance 15/09/2025 15/09/2025 https://000004.awsstudygroup.com/ 3 - Workshop: IAM Role for Application Authorization + Grant application access to AWS services + Learn why not to use access keys + Configure IAM Role on EC2 16/09/2025 16/09/2025 https://000048.awsstudygroup.com/ 4 - Workshop: Get Started with AWS Cloud9 + Create Cloud9 IDE instance + Learn basic Cloud9 operations + Use AWS CLI in Cloud9 environment 17/09/2025 17/09/2025 https://000049.awsstudygroup.com/ 5 - Workshop: Starting with Amazon S3 + Enable static website hosting + Configure public access and objects + Accelerate with CloudFront + Bucket versioning and replication 18/09/2025 18/09/2025 https://000057.awsstudygroup.com/ 6 - Workshop: Amazon RDS (Relational Database Service) + Create RDS database instance + Connect EC2 to RDS + Deploy application with database + Backup and restore 19/09/2025 19/09/2025 https://000005.awsstudygroup.com/ Week 2 Achievements: Day 2 - Amazon EC2:\nSuccessfully launched Windows Server 2022 and Amazon Linux 2023 instances Mastered EC2 fundamentals including instance types, AMIs, and EBS volumes Connected to instances using RDP (Windows) and SSH (Linux) Deployed AWS User Management Node.js application on both platforms Implemented cost governance with IAM policies Learned about EC2 security groups and network configurations Understood EC2 pricing models and cost optimization strategies Completed resource cleanup to avoid unnecessary charges Day 3 - IAM Role for Applications:\nUnderstood the security risks of using access keys and secret keys Learned best practices for granting application access to AWS services Successfully configured IAM Roles for EC2 instances Implemented secure application authorization without hardcoded credentials Compared access key method vs IAM Role method Understood the principle of temporary security credentials Applied least privilege access for application permissions Day 4 - AWS Cloud9:\nCreated and configured Cloud9 IDE environment Familiarized with Cloud9 web-based development interface Learned Cloud9 basic features and operations Configured code editor preferences and themes Used AWS CLI within Cloud9 terminal Understood Cloud9 integration with AWS services Practiced coding and debugging in cloud environment Learned when to use Cloud9 for AWS development Day 5 - Amazon S3:\nGained comprehensive understanding of Amazon S3 object storage Created S3 buckets and uploaded objects Enabled and configured static website hosting on S3 Managed public access settings and bucket policies Configured public objects with proper permissions Accelerated website delivery using CloudFront CDN Implemented bucket versioning for data protection Configured S3 Cross-Region Replication (CRR) Learned S3 storage classes and lifecycle policies Understood S3 best practices and security considerations Day 6 - Amazon RDS:\nMastered Amazon RDS core concepts and supported database engines Understood RDS storage options (General Purpose, Provisioned IOPS) Created RDS database instance with proper configuration Configured VPC security groups for database access Connected EC2 application to RDS database Deployed web application with RDS backend Implemented automated backups and point-in-time recovery Learned about Multi-AZ deployments for high availability Understood Read Replicas for read scaling Practiced backup and restore operations Applied RDS security best practices (encryption, IAM authentication) Overall Achievements:\nCompleted 5 comprehensive AWS workshops on core services Gained hands-on experience with compute, storage, and database services Mastered secure application deployment patterns Developed practical skills in EC2, IAM, Cloud9, S3, and RDS Built end-to-end applications using AWS managed services Ready to design and implement scalable AWS architectures "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Amazon Lightsail - Cost Optimization + Deploy database on Lightsail + Deploy WordPress, Prestashop, Akaunting + Application security \u0026amp; snapshots + Create alarms and scale instances 22/09/2025 22/09/2025 https://000045.awsstudygroup.com/ 3 - Workshop: Amazon Lightsail Container + Learn about Lightsail Container service + Create container service + Deploy public Docker images + Deploy custom images 23/09/2025 23/09/2025 https://000046.awsstudygroup.com/ 4 - Workshop: Auto Scaling Group with Load Balancer + Create Launch Template + Setup Application Load Balancer + Configure Auto Scaling Group + Test scalability 24/09/2025 24/09/2025 https://000006.awsstudygroup.com/ 5 - Workshop: AWS CloudWatch Monitoring + CloudWatch Metrics + CloudWatch Logs + CloudWatch Alarms + CloudWatch Dashboards 25/09/2025 25/09/2025 https://000008.awsstudygroup.com/ 6 - Workshop: Hybrid DNS with Route 53 Resolver + Setup Route 53 Resolver + Deploy Microsoft AD + Configure inbound/outbound endpoints + Setup hybrid DNS architecture 26/09/2025 26/09/2025 https://000010.awsstudygroup.com/ Week 3 Achievements: Day 2 - Amazon Lightsail:\nMastered Amazon Lightsail as a simplified AWS compute service Successfully deployed Lightsail managed database Deployed WordPress blog/website application on Lightsail Deployed Prestashop e-commerce platform Deployed Akaunting financial application Implemented application security best practices Created snapshots for backup and disaster recovery Scaled instances to larger sizes for business growth Configured CloudWatch alarms for proactive monitoring Understood Lightsail cost optimization strategies Completed within free tier limits (720 hours) Day 3 - Lightsail Container:\nUnderstood Lightsail Container service capabilities Learned containerization benefits and Docker fundamentals Created Lightsail Container service Deployed public Docker images from DockerHub Built and deployed custom Docker images Configured container service scaling and capacity Understood container networking and load balancing Managed containerized applications with simple interface Learned when to use Lightsail Container vs EC2/ECS Day 4 - Auto Scaling Group:\nMastered EC2 Auto Scaling Group architecture Created Launch Templates for instance configuration Configured Application Load Balancer for traffic distribution Set up Target Groups and health checks Created Auto Scaling Group with scaling policies Implemented dynamic scaling based on workload Tested fault tolerance and high availability Understood scaling metrics and thresholds Applied AWS Well-Architected Framework principles Optimized costs with automatic capacity adjustment Day 5 - AWS CloudWatch:\nGained comprehensive understanding of CloudWatch monitoring Worked with CloudWatch Metrics for performance tracking Configured CloudWatch Logs for centralized logging Created CloudWatch Alarms for automated alerting Built CloudWatch Dashboards for visualization Understood metric granularity (1-second resolution) Learned about metric retention (15 months) Implemented Container Insights for containerized apps Configured automated actions based on alarms Applied best practices for observability and monitoring Reduced MTTR (Mean Time To Resolution) with proactive alerts Day 6 - Route 53 Hybrid DNS:\nUnderstood Route 53 DNS service capabilities Learned about Route 53 Resolver architecture Configured inbound endpoints for on-premise queries Set up outbound endpoints for AWS to on-premise resolution Created Route 53 Resolver Rules for domain forwarding Deployed Microsoft Active Directory on AWS Connected to RDGW (Remote Desktop Gateway) Implemented hybrid DNS architecture Integrated on-premise DNS with AWS Route 53 Understood DNS query flow in hybrid environments Applied best practices for hybrid cloud connectivity Overall Achievements:\nCompleted 5 advanced AWS workshops on scalability and monitoring Mastered cost-optimized compute with Lightsail Developed containerization skills with Lightsail Container Built highly available architectures with Auto Scaling and Load Balancing Implemented comprehensive monitoring with CloudWatch Configured hybrid cloud DNS with Route 53 Resolver Ready to design and deploy production-grade AWS solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Getting Started with AWS CLI + Install and configure AWS CLI + Work with S3, SNS, IAM via CLI + Manage VPC and EC2 through CLI + Troubleshooting CLI issues 29/09/2025 29/09/2025 https://000011.awsstudygroup.com/ 3 - Workshop: Work with Amazon DynamoDB + Learn DynamoDB fundamentals + Practice Python with DynamoDB + Use AWS SDK for CRUD operations + Work with tables and indexes 30/09/2025 30/09/2025 https://000060.awsstudygroup.com/ 4 - Workshop: Amazon ElastiCache - Redis + Learn ElastiCache Redis basics + Create ElastiCache cluster + Use AWS SDK to read/write data + Optimize cache performance 01/10/2025 01/10/2025 https://000061.awsstudygroup.com/ 5 - Workshop: AWS Networking and Content Delivery + Deep dive VPC components + Transit Gateway and Site-to-Site VPN + VPC Endpoints and Peering + Route53 DNS configuration 02/10/2025 02/10/2025 https://000092.awsstudygroup.com/ 6 - Workshop: CloudFront with S3 Bucket Origin + Create S3 bucket for static hosting + Configure CloudFront distribution + Accelerate content delivery + Secure with OAI 03/10/2025 03/10/2025 https://000094.awsstudygroup.com/ Week 4 Achievements: Day 2 - AWS CLI:\nMastered AWS Command Line Interface for efficient cloud management Successfully installed AWS CLI on Windows, Linux, and macOS Configured AWS CLI with profiles for multiple environments Learned CLI configuration: Access Keys, Secret Keys, regions, output formats Managed S3 buckets and objects using CLI commands Created and configured SNS topics and subscriptions via CLI Performed IAM operations: users, groups, roles, policies Managed VPC resources: subnets, route tables, security groups Created and configured EC2 instances through CLI Automated AWS tasks with CLI scripts Troubleshot common CLI issues and errors Understood CLI output formats: JSON, YAML, text, table Day 3 - Amazon DynamoDB:\nGained comprehensive understanding of DynamoDB NoSQL database Learned DynamoDB core concepts: tables, items, attributes Understood partition keys and sort keys Mastered DynamoDB data types and naming rules Practiced Python programming with AWS SDK (Boto3) Implemented CRUD operations on DynamoDB tables Created and queried DynamoDB indexes (GSI, LSI) Optimized DynamoDB performance and throughput Understood DynamoDB pricing models Applied best practices for NoSQL data modeling Day 4 - Amazon ElastiCache Redis:\nUnderstood ElastiCache as managed in-memory data store Learned Redis fundamentals and use cases Created ElastiCache Redis cluster Configured cluster parameters and node types Connected applications to ElastiCache endpoint Used AWS SDK to write and read data from Redis Implemented caching strategies for performance Understood Redis data structures (strings, lists, sets, hashes) Configured security groups for ElastiCache access Monitored ElastiCache performance metrics Applied caching best practices for scalability Day 5 - AWS Networking and Content Delivery:\nMastered comprehensive AWS networking architecture Deep dive into VPC components: subnets, route tables, IGW, NAT Gateway Understood Availability Zones and Regions Configured Elastic Network Interfaces (ENI) and Elastic IPs Implemented VPC Endpoints for private AWS service access Configured Security Groups and Network ACLs Set up Transit Gateway for centralized connectivity Configured Site-to-Site VPN connections Implemented VPC Peering for multi-VPC communication Configured Route53 DNS endpoints and hosted zones Set up VPC Endpoint Services for private connectivity Managed Transit Gateway Network Manager Understood AWS Direct Connect architecture Configured Network Load Balancer (Layer 4) Day 6 - CloudFront with S3:\nUnderstood Amazon CloudFront CDN service Created S3 bucket for static website hosting Uploaded static content (HTML, CSS, JS) to S3 Configured CloudFront distribution with S3 origin Implemented Origin Access Identity (OAI) for security Configured CloudFront cache behaviors Set up custom domain with Route53 Enabled HTTPS with SSL/TLS certificates Optimized content delivery with edge locations Understood CloudFront pricing model Monitored CloudFront performance and metrics Applied best practices for global content delivery Overall Achievements:\nCompleted 5 advanced AWS workshops on CLI, databases, and networking Mastered AWS CLI for infrastructure automation Developed skills with NoSQL (DynamoDB) and caching (Redis) Built expertise in advanced AWS networking concepts Implemented global content delivery with CloudFront CDN Ready to architect complex, distributed AWS solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: VM Import/Export + Setup VMware Workstation environment + Import VM images to Amazon EC2 + Export EC2 instances back to on-premises + Manage VM migration lifecycle 06/10/2025 06/10/2025 https://000014.awsstudygroup.com/ 3 - Workshop: Database Schema Conversion \u0026amp; Migration + Use AWS Schema Conversion Tool (SCT) + Configure AWS Database Migration Service (DMS) + Perform heterogeneous database migration + Monitor and troubleshoot migrations 07/10/2025 07/10/2025 https://000043.awsstudygroup.com/ 4 - Workshop: AWS Elastic Disaster Recovery + Configure DRS settings and IAM + Install DRS agent on source servers + Configure EC2 Launch Templates + Perform failover testing 08/10/2025 08/10/2025 https://000100.awsstudygroup.com/ 5 - Workshop: Optimizing EC2 Costs with Lambda + Create instance tags for automation + Configure IAM roles for Lambda + Create Lambda functions for auto start/stop + Schedule with EventBridge 09/10/2025 09/10/2025 https://000022.awsstudygroup.com/ 6 - Workshop: Getting Started with Grafana + Install Grafana on Linux EC2 + Configure data sources + Create monitoring dashboards + Visualize CloudWatch metrics 10/10/2025 10/10/2025 https://000029.awsstudygroup.com/ Week 5 Achievements: Day 2 - VM Import/Export:\nMastered VM Import/Export for hybrid cloud migrations Set up VMware Workstation virtualization environment Learned to export VM images from on-premises Imported virtual machine images to Amazon EC2 Configured VM conversion settings and parameters Uploaded VM images to S3 for import process Exported EC2 instances back to on-premises format Understood VM compatibility requirements Managed VM migration lifecycle and dependencies Applied best practices for VM migrations Enabled disaster recovery and backup scenarios Day 3 - Database Migration:\nGained expertise in AWS Database Migration Service (DMS) Mastered AWS Schema Conversion Tool (SCT) Performed heterogeneous database migrations Converted database schemas between different engines Configured DMS source and target endpoints Set up serverless DMS replication Implemented continuous data replication Monitored migration tasks and metrics Troubleshot common migration issues Minimized downtime during production migrations Migrated from on-premises to RDS/Aurora Converted stored procedures and functions Day 4 - AWS Elastic Disaster Recovery:\nUnderstood AWS Elastic Disaster Recovery (DRS) service Configured DRS settings and replication Created DRS IAM users and roles Installed DRS agent on source servers (Windows/Linux) Configured continuous block-level replication Set up EC2 Launch Templates for recovery Performed drill and recovery failover operations Tested recovery point objectives (RPO) and recovery time objectives (RTO) Monitored replication lag and health Implemented point-in-time recovery Minimized downtime and data loss Applied DR best practices for business continuity Day 5 - Lambda Cost Optimization:\nImplemented EC2 cost optimization with Lambda Created instance tagging strategy for automation Configured IAM roles with proper permissions for Lambda Developed Lambda functions for EC2 start/stop automation Scheduled Lambda execution with EventBridge (CloudWatch Events) Filtered instances by tags for selective automation Implemented time-based instance scheduling Calculated cost savings from automated schedules Learned about Savings Plans for 24/7 instances Applied cost optimization strategies Monitored Lambda execution and errors Reduced unnecessary EC2 runtime costs Day 6 - Grafana Monitoring:\nInstalled and configured Grafana on Linux EC2 Set up Grafana service and security Integrated Grafana with CloudWatch data source Created custom monitoring dashboards Visualized AWS metrics and logs Configured dashboard panels and queries Set up alerts and notifications Monitored EC2, RDS, and other AWS resources Created time-series visualizations Implemented real-time monitoring Applied dashboard best practices Enhanced observability with Grafana Overall Achievements:\nCompleted 5 advanced workshops on migration, DR, and optimization Mastered VM and database migration to AWS Implemented disaster recovery with AWS DRS Automated cost optimization with Lambda Built comprehensive monitoring with Grafana Ready to design resilient, cost-effective cloud architectures "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: AWS CloudWatch Deep Dive + Configure CloudWatch Metrics + Set up CloudWatch Logs + Create CloudWatch Alarms + Build CloudWatch Dashboards 13/10/2025 13/10/2025 https://000036.awsstudygroup.com/ 3 - Workshop: Manage Resources with Tags and Resource Groups + Apply tags to AWS resources + Create tag-based queries + Build Resource Groups + Automate tasks on resource groups 14/10/2025 14/10/2025 https://000027.awsstudygroup.com/ 4 - Workshop: Manage EC2 Access with Resource Tags via IAM + Implement least privilege principle + Create IAM policies with conditions + Configure tag-based access control + Verify policies 15/10/2025 15/10/2025 https://000028.awsstudygroup.com/ 5 - Workshop: AWS Systems Manager - Patch Manager \u0026amp; Run Command + Configure Patch Manager + Create maintenance windows + Execute Run Command + Automate patching 16/10/2025 16/10/2025 https://000031.awsstudygroup.com/ 6 - Workshop: AWS Systems Manager - Session Manager + Connect to instances without SSH + Configure session logging to S3 + Implement port forwarding + Secure remote access 17/10/2025 17/10/2025 https://000058.awsstudygroup.com/ Week 6 Achievements: Day 2 - CloudWatch Deep Dive:\nMastered comprehensive CloudWatch monitoring and observability Configured CloudWatch Metrics for performance tracking Collected custom metrics from applications Set up CloudWatch Logs for centralized log management Created log groups and log streams Configured CloudWatch Alarms with multiple thresholds Set up alarm actions with SNS notifications Built comprehensive CloudWatch Dashboards Visualized metrics with graphs and widgets Monitored containerized applications with Container Insights Understood metric retention (15 months) Applied monitoring best practices for production systems Day 3 - Resource Tags and Groups:\nMastered AWS resource tagging strategy Applied tags to EC2, S3, and other AWS resources Created consistent tagging taxonomy (purpose, owner, environment) Built tag-based queries for resource discovery Created AWS Resource Groups for logical organization Automated tasks on resource groups Managed resources at scale with tags Implemented cost allocation tags Used CloudFormation stack-based queries Applied resource organization best practices Simplified multi-resource management Day 4 - IAM with Resource Tags:\nImplemented IAM least privilege principle Created IAM policies with conditional statements Configured tag-based access control (TBAC) Created IAM roles for EC2 administrators Defined policies with tag conditions Restricted resource creation based on tags Configured MFA for IAM users Implemented assume role functionality Verified policy effectiveness with testing Applied decentralized administration model Enhanced security with attribute-based access control Day 5 - Systems Manager Patch \u0026amp; Run Command:\nMastered AWS Systems Manager capabilities Configured Patch Manager for automated patching Created patch baselines for OS updates Set up maintenance windows for patching schedules Executed Run Command across multiple instances Automated tasks without SSH/RDP access Created resource groups for Systems Manager Monitored patch compliance status Centralized operational data from multiple services Automated repetitive administrative tasks Applied patch management best practices Day 6 - Session Manager:\nUnderstood AWS Systems Manager Session Manager Connected to EC2 instances without SSH keys or bastion hosts Configured IAM roles for Session Manager access Set up session logging to S3 buckets Enabled session audit trails with CloudWatch Logs Implemented port forwarding for secure tunneling Connected to private instances without public IPs Managed sessions through browser-based shell Enhanced security by eliminating inbound ports Centralized access control with IAM policies Applied secure remote access best practices Overall Achievements:\nCompleted 5 workshops on monitoring, governance, and automation Mastered CloudWatch for comprehensive observability Implemented resource organization with tags and groups Secured access with tag-based IAM policies Automated operations with Systems Manager Established secure remote access with Session Manager Ready to implement enterprise-grade AWS governance "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: AWS CloudFormation + Learn IaC with CloudFormation + Create CloudFormation templates + Deploy stacks with YAML/JSON + Use Cloud9 IDE 20/10/2025 20/10/2025 https://000037.awsstudygroup.com/ 3 - Workshop: CDK Basic + Understand AWS CDK concepts + Deploy infrastructure with code + Configure EC2 with user data + Use TypeScript/Python for IaC 21/10/2025 21/10/2025 https://000038.awsstudygroup.com/ 4 - Workshop: CDK Basic 2 (Advanced) + Build API Gateway architecture + Deploy ECS and ALB + Create Lambda functions + Implement nested stacks 22/10/2025 22/10/2025 https://000076.awsstudygroup.com/ 5 - Workshop: Introduction to Infrastructure as Code + Compare IaC frameworks + Create Lambda functions + Build VPC and EC2 + Deploy three-tier architecture 23/10/2025 23/10/2025 https://000102.awsstudygroup.com/ 6 - Workshop: EC2 Right Sizing with CloudWatch + Configure CloudWatch Agent + Collect memory metrics + Use AWS Compute Optimizer + Apply sizing best practices 24/10/2025 24/10/2025 https://000032.awsstudygroup.com/ Week 7 Achievements: Day 2 - AWS CloudFormation:\nMastered Infrastructure as Code (IaC) concepts with CloudFormation Created CloudFormation templates in YAML and JSON formats Deployed and managed infrastructure through stacks Used AWS Cloud9 as web-based IDE for development Automated resource provisioning and management Implemented basic CloudFormation features Explored advanced CloudFormation capabilities Understood CloudFormation stack lifecycle management Applied IaC best practices for infrastructure deployment Learned to version control infrastructure code Day 3 - CDK Basic:\nUnderstood AWS Cloud Development Kit (CDK) fundamentals Deployed infrastructure using programming languages Used TypeScript/JavaScript for CDK development Created CDK templates with familiar programming constructs Configured EC2 instances through user data Updated and modified CDK templates Understood CDK to CloudFormation compilation process Leveraged CDK constructs for faster development Applied object-oriented principles to infrastructure Integrated CDK with existing CloudFormation knowledge Day 4 - CDK Basic 2 (Advanced):\nBuilt complex application architecture with CDK Implemented API Gateway for RESTful APIs Deployed Elastic Load Balancer (ALB) configurations Created Elastic Container Service (ECS) deployments Integrated Lambda functions with CDK Implemented nested stack patterns Connected S3 buckets with Lambda triggers Designed multi-tier architectures with CDK Used CDK 2.151.0 for advanced features Applied architectural best practices with IaC Day 5 - Infrastructure as Code Introduction:\nCompared different IaC frameworks and methodologies Understood benefits of IaC for increased productivity Created Lambda functions through IaC Built VPC and EC2 infrastructure programmatically Deployed three-tier web architecture Managed infrastructure through source code Applied version control to infrastructure definitions Automated deployment and management processes Understood IaC advantages over manual provisioning Prepared for enterprise-scale IaC implementations Day 6 - EC2 Right Sizing:\nImplemented EC2 resource optimization with CloudWatch Created IAM roles for CloudWatch Agent Installed and configured CloudWatch Agent on EC2 Collected memory usage metrics (GB consumed) Analyzed CloudWatch data points for sizing decisions Used AWS Compute Optimizer for recommendations Applied right-sizing best practices for EC2 Optimized compute configurations based on metrics Reduced costs through proper instance sizing Implemented continuous monitoring for optimization Overall Week 7 Results:\nCompleted 5 workshops on Infrastructure as Code Mastered CloudFormation and CDK for infrastructure deployment Built advanced architectures with API Gateway, ECS, Lambda Understood IaC frameworks and methodologies Optimized EC2 resources with CloudWatch monitoring Ready to implement enterprise-grade IaC solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: AWS Web Application Firewall (WAF) + Learn WAF basics + Create WAF rules + Configure web ACLs + Protect web applications 27/10/2025 27/10/2025 https://000026.awsstudygroup.com/ 3 - Workshop: Encrypt at Rest with AWS KMS + Create KMS keys + Encrypt S3 objects + Configure CloudTrail logging + Query logs with Athena 28/10/2025 28/10/2025 https://000033.awsstudygroup.com/ 4 - Workshop: AWS Secrets Manager with RDS and Fargate + Store RDS credentials + Configure secret rotation + Access secrets from Fargate + Test with shell scripts 29/10/2025 29/10/2025 https://000096.awsstudygroup.com/ 5 - Workshop: Implementing AWS Cognito Across Sites + Create Cognito User Pools + Configure Identity Pools + Implement authentication + Deploy cross-site integration 30/10/2025 30/10/2025 https://000141.awsstudygroup.com/ 6 - Workshop: Autonomous Patching with EC2 Image Builder + Configure EC2 Image Builder + Create AMI pipeline + Automate with Systems Manager + Blue/green deployment 31/10/2025 31/10/2025 https://000099.awsstudygroup.com/ Week 8 Achievements: Day 2 - AWS Web Application Firewall:\nMastered AWS WAF fundamentals for web application protection Created and configured WAF web ACLs Implemented WAF rules for traffic filtering Protected applications from common web exploits Configured rate-based rules to prevent DDoS attacks Used AWS managed rule groups Created custom rule groups for specific requirements Monitored WAF metrics and logs Applied WAF best practices for security Integrated WAF with CloudFront and ALB Day 3 - KMS Encryption at Rest:\nUnderstood AWS Key Management Service (KMS) concepts Created and managed customer master keys (CMKs) Encrypted S3 objects with KMS keys Configured server-side encryption (SSE-KMS) Set up CloudTrail for KMS API logging Used Amazon Athena to query CloudTrail logs Implemented key rotation policies Shared encrypted data securely between accounts Applied encryption best practices for data at rest Managed key policies and grants Ensured compliance with encryption requirements Day 4 - AWS Secrets Manager Integration:\nMastered AWS Secrets Manager for credential management Stored RDS database credentials securely Configured automatic secret rotation Integrated Secrets Manager with Amazon RDS Accessed secrets from EC2 instances Retrieved secrets from AWS Fargate containers Implemented least privilege access with IAM Created Docker containers with secret integration Automated secret management with shell scripts Applied preventative security controls (CSF framework) Built secure VPC architecture with private subnets Day 5 - AWS Cognito Authentication:\nUnderstood Amazon Cognito identity platform Created and configured Cognito User Pools Set up Cognito Identity Pools for AWS access Implemented user authentication for web/mobile apps Integrated third-party identity providers (IdPs) Configured SAML 2.0 and OIDC authentication Issued authenticated JSON Web Tokens (JWTs) Implemented cross-site authentication Used role-based and attribute-based access control Applied OAuth 2.0 authorization flows Built secure user directory with self-service features Day 6 - Autonomous Patching Pipeline:\nBuilt automated patching solution with EC2 Image Builder Created AMI builder pipelines Configured Systems Manager Automation documents Implemented blue/green deployment methodology Used CloudFormation AutoScalingReplacingUpdate policy Automated OS patching for security compliance Reduced operational overhead with automation Ensured minimal interruption to application availability Established traceability for compliance audits Deployed updated AMIs to application clusters Applied Well-Architected security best practices Overall Week 8 Results:\nCompleted 5 workshops on AWS Security and Automation Mastered web application protection with WAF Implemented encryption at rest with KMS Secured credentials with Secrets Manager Built authentication systems with Cognito Automated patching with EC2 Image Builder Ready to implement enterprise security solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Deploy AWS Backup to the System + Create backup plans + Automate backups for EBS, RDS, DynamoDB + Test restore operations + Configure SNS notifications 03/11/2025 03/11/2025 https://000013.awsstudygroup.com/ 3 - Workshop: Setting up VPC Peering + Create VPC Peering connections + Configure Network ACLs + Update route tables + Enable cross-peer DNS 04/11/2025 04/11/2025 https://000019.awsstudygroup.com/ 4 - Workshop: Set up AWS Transit Gateway + Create Transit Gateway + Configure TGW attachments + Set up TGW route tables + Connect multiple VPCs 05/11/2025 05/11/2025 https://000020.awsstudygroup.com/ 5 - Workshop: Event-Driven Architecture with SNS and SQS + Create SNS topics + Configure SQS queues + Implement message filtering + Build pub/sub patterns 06/11/2025 06/11/2025 https://000077.awsstudygroup.com/ 6 - Workshop: Secure Data Sharing with EBS NVMe Reservation + Configure EBS Multi-Attach + Implement NVMe reservations + Set up PostgreSQL backup + Test data recovery 07/11/2025 07/11/2025 https://100000.awsstudygroup.com/ Week 9 Achievements: Day 2 - AWS Backup Implementation:\nMastered AWS Backup for centralized data protection Created automated backup plans for AWS resources Configured backups for EBS Volumes, RDS Databases, DynamoDB Tables Set up backup for EFS File Systems Tested backup and restore operations Configured AWS SNS for backup notifications Implemented backup policies from a single location Automated data protection workflows Ensured data recovery capabilities Applied backup best practices for business continuity Validated backups through restore testing Day 3 - VPC Peering:\nUnderstood VPC Peering concepts and architecture Created VPC Peering connections between VPCs Configured Network ACLs for stateless filtering Updated route tables for peering communication Enabled cross-peering DNS for name resolution Implemented security groups and NACLs together Used private IPv4/IPv6 addresses for routing Enhanced security by avoiding public internet traversal Reduced latency with direct VPC connections Applied defense-in-depth security strategies Understood non-transitive peering limitations Day 4 - AWS Transit Gateway:\nMastered AWS Transit Gateway for scalable networking Created Transit Gateway as cloud router hub Configured Transit Gateway Attachments for VPCs Set up Transit Gateway Route Tables Connected multiple VPCs through single gateway Simplified network architecture vs VPC Peering Implemented centralized network management Reduced operational complexity Understood TGW vs VPC Peering trade-offs Applied transit gateway for enterprise networking Configured encryption in transit Day 5 - Event-Driven Architecture:\nBuilt event-driven architectures with SNS and SQS Created Amazon SNS topics for pub/sub messaging Configured Amazon SQS queues for message processing Implemented message filtering with filter policies Designed simple pub/sub patterns Applied advanced message filtering techniques Used JSON attributes for message differentiation Offloaded message routing logic from publishers Enabled independent development team operations Improved application scalability with events Built decoupled microservices architecture Day 6 - EBS NVMe Reservation:\nImplemented secure data sharing with EBS NVMe reservations Configured EBS Multi-Attach for io2 volumes Used NVMe reservations for storage fencing Maintained data consistency across instances Set up PostgreSQL on multiple EC2 instances Implemented database backup configurations Tested data recovery across VPCs Applied industry-standard storage fencing protocols Coordinated access control for shared volumes Worked with SUSE Linux, RHEL, and Amazon Linux 2 Enhanced security with separate VPCs Optimized performance with efficient data management Overall Week 9 Results:\nCompleted 5 workshops on Networking, Backup, and Event-Driven Architecture Mastered data protection with AWS Backup Built scalable network architectures with VPC Peering and Transit Gateway Implemented event-driven systems with SNS/SQS Secured data sharing with EBS NVMe reservations Ready to design enterprise-grade cloud architectures "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will find a comprehensive 12-week worklog documenting my AWS learning journey through the First Cloud Journey program. This worklog spans 3 months of intensive hands-on practice with AWS services, covering fundamental concepts to advanced serverless architectures.\nProgram Overview This worklog documents my progression through 12 weeks of AWS workshops, completing 60 hands-on labs covering various AWS services and architectural patterns. Each week focused on specific themes, building knowledge progressively from foundational services to enterprise-grade solutions.\nWeekly Progress Week 1: AWS Fundamentals - Account Setup, Budgets, Support Plans, IAM, and VPC\nCreated AWS account and configured billing alerts Explored AWS Support plans and best practices Mastered IAM user management and permissions Built foundational VPC networking infrastructure Week 2: Core Compute and Storage - EC2, IAM Roles, Cloud9, S3, and RDS\nDeployed and managed EC2 instances Implemented IAM roles for secure resource access Set up Cloud9 development environment Configured S3 buckets and RDS databases Week 3: Application Deployment - Lightsail, Containers, Auto Scaling, Monitoring, and DNS\nLaunched applications on Amazon Lightsail Worked with Docker containers and ECR Configured Auto Scaling for high availability Set up CloudWatch monitoring and Route 53 DNS Week 4: CLI and Database Services - AWS CLI, DynamoDB, ElastiCache, Networking, and CDN\nMastered AWS CLI for automation Built NoSQL applications with DynamoDB Implemented caching with ElastiCache Configured CloudFront for content delivery Week 5: Migration and Disaster Recovery - VM Import/Export, Database Migration, DRS, Lambda, and Grafana\nMigrated VMs and databases to AWS Implemented disaster recovery solutions Optimized Lambda function costs Set up Grafana for observability Week 6: Monitoring and Governance - CloudWatch, Resource Tags, IAM Policies, and Systems Manager\nDeployed comprehensive CloudWatch monitoring Organized resources with tags and groups Implemented tag-based access control Automated operations with Systems Manager Week 7: Infrastructure as Code - CloudFormation, CDK, and EC2 Optimization\nBuilt infrastructure with CloudFormation templates Developed with AWS CDK using TypeScript/Python Created nested stacks and advanced patterns Optimized EC2 sizing with Compute Optimizer Week 8: Security and Automation - WAF, KMS, Secrets Manager, Cognito, and Image Builder\nProtected applications with AWS WAF Encrypted data with KMS Managed secrets securely Implemented authentication with Cognito Automated patching with EC2 Image Builder Week 9: Networking and Messaging - AWS Backup, VPC Peering, Transit Gateway, SNS/SQS, and EBS Multi-Attach\nAutomated backups for business continuity Connected VPCs with Peering and Transit Gateway Built event-driven architectures with SNS/SQS Shared data securely with EBS NVMe reservations Week 10: CI/CD and Serverless - CodePipeline, ECS/EKS Deployments, DynamoDB, and Step Functions\nBuilt CI/CD pipelines for containers Deployed to ECS and EKS with automation Explored advanced DynamoDB patterns Orchestrated workflows with Step Functions Week 11: Serverless Applications - CodeStar, Lambda Functions, API Gateway, Cognito, and Messaging\nAutomated releases with AWS CodeStar Built serverless backends with Lambda Created RESTful APIs with API Gateway Implemented user authentication Processed orders with SQS and SNS Week 12: Advanced Serverless - CI/CD, Document Management, Amplify, SSL/TLS, and Monitoring\nAutomated serverless deployments Built document management system Integrated Amplify for auth and storage Secured websites with SSL certificates Monitored with CloudWatch and X-Ray Key Achievements Completed 60 hands-on AWS workshops across 12 weeks Gained expertise in 25+ AWS services Built production-ready architectures including serverless, containerized, and traditional applications Mastered Infrastructure as Code with CloudFormation and CDK Implemented enterprise-grade security, monitoring, and CI/CD practices Ready to design and deploy scalable cloud solutions "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS First Cloud AI Journey – Leaf E-commerce Project Plan 1. BACKGROUND and Motivation 1.1 Executive Summary Leaf is an e-commerce platform specializing in fashion products for men and women, including fashion accessories such as jewelry, shoes, hats, and leather straps. The website integrates AWS services to optimize costs and enhance the user experience.\n1.2 Project Success Criteria Fully functional e-commerce platform with AWS integration. Optimized cost and performance using serverless architecture. Fast page load with CDN (CloudFront) and S3 hosting. Seamless notifications using SNS/SES. AI-powered translation and assistance (optional) via Amazon Translate \u0026amp; Bedrock. 1.3 Assumptions Customers have basic knowledge of cloud services and AWS accounts. Serverless approach is acceptable; no dedicated servers required. Traffic is moderate (~few thousand users/month) and costs are expected to be low. Required services (S3, Lambda, DynamoDB, etc.) are accessible in selected AWS regions. Images and static assets will be stored in S3/CDN for performance. 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The platform applies an AWS Serverless architecture to manage data.\nComponents and Roles in the AWS Architecture A. User Interface Layer Service Role Detailed Description AWS Amplify Website deployment Hosts static websites (React, Vue, Next.js) and automatically builds/deploys when code is pushed to GitHub. Amazon CloudFront (CDN) Improve page loading speed Caches static content close to users to reduce latency. Amazon S3 Store static files \u0026amp; product images Acts as a content repository for images, banners, CSS/JS files. B. Application Logic Layer Service Role Detailed Description Amazon API Gateway API gateway Receives requests from frontend and forwards them to Lambda functions for processing. AWS Lambda Server-side logic Handles orders, payments, authentication, email without dedicated servers. Amazon DynamoDB NoSQL database Stores products, accounts, orders, shopping carts. AWS Secrets Manager Secure sensitive data Stores API keys, payment tokens, database passwords. C. User Management \u0026amp; Security Layer Service Role Detailed Description AWS WAF Web protection Protects against SQL Injection, XSS, DDoS. Amazon Route 53 DNS \u0026amp; domain Manages domain names. D. Notification \u0026amp; Communication Layer Service Role Detailed Description Amazon SNS System notifications Sends notifications to admins or users. Amazon SES Transactional emails Sends order confirmations, promotions, password reset emails. E. AI \u0026amp; Machine Learning Layer Service Role Detailed Description Amazon Translate Content translation Translates product descriptions to other languages. Amazon Bedrock AI content generation Creates chatbots for shopping assistance. F. Monitoring \u0026amp; Management Layer Service Role Detailed Description Amazon CloudWatch System monitoring Monitors logs, performance, alerts for errors or cost spikes. AWS CloudTrail Administrative logging Tracks configuration changes for auditing purposes. 2.2 Technical Plan Collect system requirements and features. Estimate cost and check feasibility. Design UI prototypes using Figma. Build database schema and backend APIs. Develop frontend interface. Integrate AWS services (S3, Lambda, DynamoDB, etc.). Test and deploy system using serverless architecture. 2.3 Project Plan Agile Scrum framework, 8 × 2-week sprints. Sprint Reviews and Retrospectives conducted with stakeholders. Knowledge transfer sessions scheduled at end of each sprint. 2.4 Security Considerations Enable MFA on account access. Configure AWS CloudTrail \u0026amp; Config for monitoring. Apply WAF to block malicious requests. Encrypt sensitive data using Secrets Manager \u0026amp; AWS KMS. 3. Activities AND Deliverables 3.1 Activities and Deliverables Project Phase Timeline Activities Deliverables/Milestones Total man-day Assessment Week 1-2 Collect requirements, estimate costs Requirement document X man-day Setup Base Infrastructure Week 3-4 Provision S3, Amplify, CloudFront Working base environment X man-day Setup Components Week 5-6 API Gateway, Lambda, DynamoDB Backend \u0026amp; auth ready X man-day Testing \u0026amp; Go-live Week 7 Full integration testing Live system deployed X man-day Handover Week 8 Knowledge transfer \u0026amp; documentation Final deliverables accepted X man-day 3.2 Out of Scope Non-AWS hosting. Legacy system migrations. Custom AI/ML development beyond Translate/Bedrock. 3.3 Path to Production POC built for main use-cases. Production setup requires tuning for operational excellence. Error handling, logging, and testing fully implemented. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Service Group Total Cost (USD/month) Storage \u0026amp; Data 3.55 Backend \u0026amp; Processing 0.75 UI \u0026amp; Security 8.20 Email \u0026amp; Notifications 0.20 AI \u0026amp; ML (Optional) 0.25 Monitoring \u0026amp; Logs 1.50 Total (Actual) ≈ 14.45 USD / month View AWS Pricing Calculator\n5. TEAM Name Student ID Nguyễn Tuấn Kiệt SE182120 Nguyễn Thanh Sơn SE183379 Trương Minh Khánh SE182131 Nguyễn Văn Thành SE193632 Lê Hồ Gia Bảo SE184518 "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM Permissions Create an IAM role with Permissions policies: AmazonBedrockFullAccess and CloudWatchLogsFullAccess\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/3-blogstranslated/3.2-blog2/","title":"Announcing General Availability of the AWS SDK for Swift","tags":[],"description":"","content":"Announcing General Availability of the AWS SDK for Swift by Josh Elkins | on 17 SEP 2024\nWe are excited to announce the General Availability of the AWS SDK for Swift, the AWS solution for accessing Amazon Web Services from applications built with the Swift programming language. AWS SDK for Swift provides a modern, user-friendly, and native Swift interface for accessing Amazon Web Services from Apple platforms, AWS Lambda, and Linux-based Swift on Server applications.\nAWS SDK for Swift entered Developer Preview in 2021 with the promise to bring a first-class, modern AWS experience to the Swift developer community. We\u0026rsquo;re delighted that it is now emerging from Preview with a complete AWS SDK feature set comparable to that offered on AWS SDKs for other languages. AWS SDK for Swift is ready for you to use today in your own Swift-based applications, from Apple Watch to high-powered Linux and Mac servers.\nSwift is the modern language of choice for developing natively on Apple devices, and is also widely used on Linux for server and utility purposes. AWS SDK for Swift allows app developers to add tight integration with AWS services into their native mobile and desktop apps. AWS SDK for Swift fully embraces modern Swift language features and provides access to AWS services with an interface that will feel natural to the Swift developer community.\nQuick Start The following steps show how to create a simple Swift command line tool that uses the AWS Simple Token Service (STS) client included in AWS SDK for Swift. All you need is:\na Mac with latest Xcode, or a Linux machine with Swift 5.9 or newer toolchain installed. See swift.org\u0026rsquo;s Getting Started guide if you don\u0026rsquo;t already have Swift on your machine. AWS credentials and default AWS region configured on your machine, using the AWS Command-Line Tools. In the Terminal, create a new Swift package:\n$ mkdir MySwiftApp $ cd MySwiftApp $ swift package init --name MySwiftApp Edit the Package.swift file to require the AWS SDK for Swift as a package dependency and add the AWS STS service client to your target:\n// swift-tools-version: 5.9 import PackageDescription let package = Package( name: \u0026#34;MySwiftApp\u0026#34;, platforms: [.macOS(.v10_15)], dependencies: [ .package(url: \u0026#34;https://github.com/awslabs/aws-sdk-swift.git\u0026#34;, from: \u0026#34;1.0.0\u0026#34;) ], targets: [ .executableTarget(name: \u0026#34;MySwiftApp\u0026#34;, dependencies: [ .product(name: \u0026#34;AWSSTS\u0026#34;, package: \u0026#34;aws-sdk-swift\u0026#34;) ]) ] ) Then, edit Sources/MySwiftApp/MySwiftApp.swift to create a client in code and use it to check your credentials with AWS STS:\nimport AWSSTS @main struct MySwiftApp { static func main() async throws { let client = try await STSClient() let input = GetCallerIdentityInput() let output = try await client.getCallerIdentity(input: input) print(\u0026#34;Request succeeded\u0026#34;) print(\u0026#34;User ID: \\(output.userId)\u0026#34;) print(\u0026#34;Account: \\(output.account)\u0026#34;) } } Finally, run your Swift package from the terminal, and it will install dependencies, compile, and run:\n$ swift run ... Request succeeded User ID: \u0026lt;your AWS user ID\u0026gt; Account: \u0026lt;your AWS user account ID\u0026gt; Consult the official AWS SDK for Swift Getting Started guide for comprehensive, step-by-step instructions to integrate AWS SDK for Swift into your existing Swift package or Xcode application.\nFeatures Full Support for Swift Structured Concurrency The AWS SDK for Swift utilizes Swift Structured Concurrency throughout for ease of use and efficient operation, both in public APIs and internally. No need for completion blocks and convoluted asynchronous control flow; Swift async / await on AWS operations makes it easy to write simple, concise, and correct asynchronous Swift code.\n// Create a client for listing the current user\u0026#39;s S3 buckets let client = try await S3Client() // Obtain a list of buckets. Execution suspends while the request is made // to AWS S3. let input = ListBucketsInput() let output = try await client.listBucketsV2(input: input) // Once the request completes, execution continues and the buckets are printed. print(output.buckets) Binary Data Streaming The AWS SDK for Swift effortlessly streams binary data for both requests and responses. Data can be streamed either to or from a file or an in-memory buffer. The interface for streamed binary data uses a Swift AsyncSequence to easily and efficiently transfer large binary payloads in the form of small, sequential chunks of data.\nIn the following example, a large file is streamed to AWS S3 from a file on the local filesystem.\n// Create a S3 client and a Foundation file handle referring to a local data file. let client = try await S3Client() let fileHandle = FileHandle(forReadingAtPath: \u0026#34;/tmp/my-local-file\u0026#34;)! // Create a stream with the file handle, and use it as the body of a S3 PutObject // request to create a new S3 object named my-new-file. let stream = FileStream(fileHandle: fileHandle) let body = ByteStream.stream(stream) let input = PutObjectInput(body: body, bucket: \u0026#34;amzn-s3-demo-bucket\u0026#34;, key: \u0026#34;my-new-file\u0026#34;) // Stream the data in the local file to AWS S3 and store it in a new object. // If the file is large, AWS SDK for Swift will stream it in chunks until it // has all been transferred. let _ = try await client.putObject(input: input) Event Streaming The AWS SDK for Swift also supports AWS event streaming. Event streams enable asynchronous transmission of discrete, serialized messages (\u0026ldquo;events\u0026rdquo;) over the network. Event streams can be either unidirectional, or with HTTP/2, bidirectional. Like binary data streams, event streams are delivered by a Swift AsyncSequence for easy consumption by the caller.\nIn the following bidirectional streaming example, the AWS Transcribe Streaming service is used to stream audio to the server while transcription events are returned in real-time.\n// Create a TranscribeStreaming client and prepare audio for transmission. let client = try await TranscribeStreamingClient() let input = StartStreamTranscriptionInput(audioStream: audioStream, languageCode: .enUs, mediaEncoding: .pcm, mediaSampleRateHertz: 8000) // Start the transcription request. An HTTP/2 connection will be opened. // Audio data will be streamed to the server while the server simultaneously // streams transcription events back as words are transcribed from the audio. let output = try await client.startStreamTranscription(input: input) for try await event in output.transcriptResultStream! { print(event) } Consult the documentation for individual AWS services to see which services support event streaming.\nWaiters Waiters allow the AWS SDK for Swift to suspend and wait for an expected service condition to occur, such as the creation or deletion of a resource, and automatically resume execution once the condition has been satisfied.\nIn the following example, the AWS SDK for Swift is used to wait until a previously created EC2 instance reaches the Running state.\n// An EC2 instance was previously created \u0026amp; started with the ID i-05450a63fe8ae3329. // The instance may take some time before it transitions to running. // We want to wait until our instance is running before continuing execution. let input = DescribeInstancesInput(instanceIds: [\u0026#34;i-05450a63fe8ae3329\u0026#34;]) // We want to wait no longer than 240 seconds (four minutes) for the instance to // reach the Running state. let options = WaiterOptions(maxWaitTime: 240.0) // This method call will suspend the calling thread, intermittently poll the EC2 // instance to check its state, and will resume execution once the instance // reports that it is running. // // If the maximum wait time is reached or an unexpected error is encountered while // polling, then this method call will throw an appropriate error back to the // caller. let client = try await EC2Client() let _ = try await client.waitUntilInstanceRunning(options: options, input: input) // Once this point is reached, the instance is now verified to be running. Many AWS operations support waiters. Consult the documentation for individual AWS services for more details.\nPaginators Paginators are a feature that allows the caller to automatically retrieve large sets of data in pages, without writing code to perform each individual page retrieval.\nPaginated operations return individual pages of data using a Swift AsyncSequence of discrete-sized pages of data.\nIn the following example, AWS SDK for Swift uses pagination to retrieve the list of contents of a S3 bucket containing a very large number of objects.\n// Create a S3 client and define the bucket you want to list let client = try await S3Client() let input = ListObjectsV2Input(bucket: \u0026#34;amzn-s3-demo-bucket\u0026#34;) // Get an asynchronous sequence with pages of the bucket\u0026#39;s contents let output = client.listObjectsV2Paginated(input: input) // Iterate over the sequence with a loop to get the contents a page at a time for try await page in output { print(page.contents) } Many AWS operations support pagination of their results. Consult the documentation for individual AWS services for more details.\nAutomatic Retry The AWS SDK for Swift is configured by default with automatic retry, ensuring that failed AWS requests are retried when transient network or server conditions cause recoverable errors. Retry may be customized by configuring parameters, or for greater control, by providing a custom retry strategy.\nFlexible HTTP Client Options AWS SDK for Swift is supplied with two fully-featured and highly configurable HTTP clients, to provide the right HTTP networking support across all supported platforms:\nA cross-platform HTTP client based on AWS\u0026rsquo;s Common Runtime (CRT) libraries that provides high performance and stability, even in challenging network conditions. Ideal for server-side and high-throughput Mac applications. A HTTP client based on the Apple Foundation library\u0026rsquo;s URLSession HTTP connection manager, for the best performance, power conservation and tight platform integration on Apple devices. Both clients support all AWS service features and are highly configurable to meet your application\u0026rsquo;s needs.\nBased on customer feedback, additional HTTP clients may be provided in the future.\nOther Features The AWS SDK for Swift provides everything a developer needs to fully integrate their application with their AWS services.\nInterceptors: Interceptors allow for full customization of request \u0026amp; response handling by injecting code into the request lifecycle. Logging: Fine-grained control over logging to any swift-log compatible logger. Observability: AWS SDK for Swift is provided with support for detailed metrics and tracing. You can provide a plug-in to forward data into Amazon CloudWatch or other backend collection systems. Supported Tools \u0026amp; Platforms AWS SDK for Swift supports multiple development environments:\nXcode 15.0 and higher on Mac. Xcode is available from the Mac App Store. Swift 5.9 or higher on Linux. Swift for Linux is available for install on swift.org. AWS SDK for Swift compiles to the following platforms \u0026amp; minimum OS versions:\nmacOS 10.15 iOS / iPadOS 13 tvOS 13 watchOS 9 visionOS 1 Amazon Linux 2 Ubuntu 20.04 What\u0026rsquo;s Next We\u0026rsquo;re excited to see what you\u0026rsquo;ll build with AWS SDK for Swift, and we\u0026rsquo;re here to help you integrate it into your app successfully. The following are some next steps to get you started on the right foot:\nPlease consult our Getting Started guide for detailed, step-by-step instructions on how to integrate AWS SDK for Swift into your own Swift package or Xcode application. Code for the AWS SDK for Swift is open-source and is available on Github. If you have questions about AWS SDK for Swift or have a request or suggestion, you can start a discussion on our Github page. If you believe you\u0026rsquo;ve found a bug, you can file an issue as well. Complete AWS SDK for Swift setup instructions, code samples, and more are available in the AWS SDK for Swift Developer Guide. Comprehensive documentation for the entire range of AWS services is available on the main AWS Documentation site. Tags: aws-sdk, Swift\nAuthor: Josh Elkins - Josh is a software developer for the AWS SDK for Swift.\nSource: AWS Developer Tools Blog\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Deploying CI/CD with ECS Container + Set up CI/CD for ECS + Deploy with GitLab and GitHub + Use CodeBuild integration + Monitor with Container Insights 10/11/2025 10/11/2025 https://000017.awsstudygroup.com/ 3 - Workshop: Deploy Applications to EC2 with CodePipeline + Configure CodeCommit repository + Set up CodeBuild + Deploy with CodeDeploy + Create end-to-end pipeline 11/11/2025 11/11/2025 https://000023.awsstudygroup.com/ 4 - Workshop: Amazon EKS - CI/CD with CodePipeline + Create IAM roles for EKS + Configure aws-auth ConfigMap + Set up GitHub integration + Deploy to EKS cluster 12/11/2025 12/11/2025 https://000152.awsstudygroup.com/ 5 - Workshop: Amazon DynamoDB Immersion Day + Hands-on labs for DynamoDB + Advanced design patterns + Change Data Capture + Build serverless event-driven apps 13/11/2025 13/11/2025 https://000039.awsstudygroup.com/ 6 - Workshop: Get Started with AWS Step Functions + Orchestrate workflows + Implement branching logic + Use parallel execution + Handle errors and callbacks 14/11/2025 14/11/2025 https://000047.awsstudygroup.com/ Week 10 Achievements: Day 2 - CI/CD with ECS Container:\nMastered CI/CD automation for Amazon ECS deployments Set up continuous integration with GitLab Configured GitHub Actions for container deployments Integrated AWS CodeBuild with ECS pipelines Implemented Container Insights for monitoring Configured Firelens for log routing Automated entire deployment workflow Reduced manual deployment errors Enhanced workflow efficiency with DevOps practices Applied ECS monitoring best practices Ensured application stability and performance Day 3 - CodePipeline for EC2:\nBuilt end-to-end CI/CD pipeline with AWS CodePipeline Configured AWS CodeCommit for Git-based repositories Set up AWS CodeBuild for compilation and testing Deployed Node.js applications with CodeDeploy Automated deployments to EC2 instances Configured CodeDeploy agents on EC2 Used CloudWatch Events to trigger pipelines Stored build artifacts in S3 buckets Encrypted artifacts with AWS KMS Implemented automated stage gates Applied continuous delivery best practices Day 4 - EKS CI/CD with CodePipeline:\nImplemented CI/CD for Amazon EKS cluster Created IAM roles for EKS integration Modified aws-auth ConfigMap for permissions Forked and configured GitHub repository Generated GitHub access tokens Set up CodePipeline for Kubernetes deployments Triggered automatic releases on code commits Deployed sample services to EKS Monitored automated deployments Applied EKS security best practices Integrated version control with Kubernetes Day 5 - DynamoDB Immersion Day:\nCompleted hands-on labs for Amazon DynamoDB Mastered NoSQL data modeling best practices Explored advanced DynamoDB design patterns Implemented Change Data Capture (CDC) Built serverless event-driven architecture with DynamoDB Streams Integrated Generative AI with DynamoDB and OpenSearch Used Amazon Bedrock for natural language queries Deployed global serverless applications Modeled game player data patterns Worked on design challenge scenarios Understood single-digit millisecond performance Applied DynamoDB at scale Day 6 - AWS Step Functions:\nMastered workflow orchestration with AWS Step Functions Created state machines for service coordination Implemented Task state with Lambda functions Used Choice state for branching logic Applied Parallel state for concurrent execution Implemented pause/resume with waitForTaskToken Designed error handling strategies Visualized workflows in Step Functions console Debugged and audited workflow executions Built fully managed workflow automation Applied serverless orchestration patterns Overall Week 10 Results:\nCompleted 5 workshops on CI/CD, Containers, and Serverless Mastered CI/CD pipelines for ECS, EC2, and EKS Implemented automated deployment workflows Explored advanced DynamoDB patterns and use cases Orchestrated complex workflows with Step Functions Ready to build production-grade DevOps pipelines "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Configure App Auto-Release with CodeStar + Set up AWS CodeStar project + Use CodeCommit with Eclipse IDE + Deploy to Elastic Beanstalk + Implement CI/CD pipeline 17/11/2025 17/11/2025 https://000051.awsstudygroup.com/ 3 - Workshop: Serverless Bookstore - Lambda Functions + Create Lambda functions + Handle S3 triggers + Image processing with Lambda + Write data to DynamoDB 18/11/2025 18/11/2025 https://000078.awsstudygroup.com/ 4 - Workshop: Serverless - Build Frontend with API Gateway + Deploy frontend to Amplify + Create Lambda functions + Configure API Gateway + Test APIs with Postman 19/11/2025 19/11/2025 https://000079.awsstudygroup.com/ 5 - Workshop: Serverless - Authentication with Cognito + Create Cognito User Pool + Implement sign-in/sign-up + Configure Lambda authorizers + Test authentication flow 20/11/2025 20/11/2025 https://000081.awsstudygroup.com/ 6 - Workshop: Serverless - Processing Orders with SQS and SNS + Create SQS queues + Set up SNS topics + Implement order processing + Integrate Lambda with SQS/SNS 21/11/2025 21/11/2025 https://000083.awsstudygroup.com/ Week 11 Achievements: Day 2 - CodeStar Auto-Release:\nMastered AWS CodeStar for unified CI/CD management Set up continuous delivery toolchain in minutes Configured AWS CodeCommit Git repositories Used Eclipse IDE with CodeCommit integration Implemented AWS CodeBuild for compilation and testing Created AWS CodePipeline for automated releases Deployed Java Spring application to Elastic Beanstalk Implemented CodeDeploy for Windows Service deployment Configured automatic triggers on code commits Managed team access with CodeStar dashboard Integrated JIRA for issue tracking Applied lift-and-shift migration to PaaS Day 3 - Serverless Lambda Functions:\nUnderstood serverless architecture fundamentals Created AWS Lambda functions for compute Implemented S3 event triggers Built image resizing function with Lambda Processed S3 object uploads automatically Stored data in Amazon DynamoDB Configured Lambda IAM roles and permissions Applied serverless best practices Eliminated server management overhead Built scalable applications without infrastructure Used DynamoDB for data persistence Integrated multiple AWS serverless services Day 4 - API Gateway Frontend:\nBuilt frontend web application for serverless backend Deployed static website to AWS Amplify Created Lambda functions for API endpoints Configured Amazon API Gateway Implemented RESTful APIs Connected frontend to DynamoDB via Lambda Tested APIs with Postman Integrated API Gateway with Lambda Configured CORS for frontend access Deployed end-to-end serverless web application Applied API Gateway best practices Built complete serverless architecture Day 5 - Cognito Authentication:\nImplemented authentication with Amazon Cognito Created Cognito User Pools Built sign-in and sign-up flows Implemented email verification Configured password reset functionality Used Cognito Identity Pools for AWS access Integrated third-party authentication providers Created Lambda authorizers for API Gateway Implemented JWT token validation Secured serverless APIs with authentication Applied user management best practices Built complete authentication system Day 6 - SQS and SNS Order Processing:\nBuilt order processing system with SQS and SNS Created Amazon SQS queues for message handling Configured Amazon SNS topics for notifications Implemented pub/sub messaging patterns Processed orders asynchronously with queues Sent admin notifications via SNS Created Lambda functions for order handling Stored processed orders in DynamoDB Implemented order management interface Configured message filtering and routing Applied decoupled architecture patterns Built scalable message-driven application Overall Week 11 Results:\nCompleted 5 workshops on Serverless and DevOps automation Mastered AWS Developer Tools (CodeStar, CodeCommit, CodeBuild, CodeDeploy, CodePipeline) Built complete serverless web applications Implemented authentication and authorization with Cognito Created message-driven architectures with SQS and SNS Ready to build production serverless applications "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Workshop: Serverless - CI/CD with AWS CodePipeline + Build SAM pipeline + Create frontend pipeline + Automate deployments + Test CI/CD workflow 24/11/2025 24/11/2025 https://000084.awsstudygroup.com/ 3 - Workshop: DMS - Document Management System Lambda Functions + Create DynamoDB tables + Write Lambda functions + Implement file management + Test Lambda operations 25/11/2025 25/11/2025 https://000133.awsstudygroup.com/ 4 - Workshop: Serverless - Amplify Authentication and Storage + Configure Amplify authentication + Set up S3 storage + Implement access levels + Test file uploads 26/11/2025 26/11/2025 https://000134.awsstudygroup.com/ 5 - Workshop: Serverless - Set up S3 Static SSL Website + Create custom domain with Route 53 + Request SSL certificate + Configure CloudFront CDN + Enable HTTPS 27/11/2025 27/11/2025 https://000137.awsstudygroup.com/ 6 - Workshop: Serverless - Monitoring with CloudWatch and X-Ray + Configure CloudWatch monitoring + Set up X-Ray tracing + Analyze performance + Debug applications 28/11/2025 28/11/2025 https://000140.awsstudygroup.com/ Week 12 Achievements: Day 2 - Serverless CI/CD:\nMastered CI/CD practices for serverless applications Built SAM (Serverless Application Model) pipeline Created frontend pipeline for static websites Automated deployments with AWS CodePipeline Integrated GitLab/CodeCommit with CodeBuild Configured CloudFormation for serverless updates Automated build and deployment to S3 Implemented continuous integration workflows Applied continuous delivery practices Reduced deployment time and risks Eliminated manual deployment processes Built production-ready CI/CD pipelines Day 3 - Document Management System:\nBuilt complete Document Management System (DMS) Created DynamoDB tables for file metadata Implemented Lambda functions for CRUD operations Built file upload and download functionality Implemented document search capabilities Created document deletion features Tracked file statistics (count, size, updates) Integrated with S3 for file storage Used Amplify for authentication and storage Deployed with AWS SAM Configured SSL/TLS with CloudFront Built complete serverless application Day 4 - Amplify Auth \u0026amp; Storage:\nImplemented AWS Amplify authentication Configured Cognito User Pools with Amplify Set up S3 storage with Amplify Implemented file upload to S3 Configured storage access levels Managed user authentication flows Integrated Amplify library with frontend Built secure file management system Applied access control patterns Simplified authentication implementation Used Amplify CLI for configuration Built production-ready auth system Day 5 - SSL Website with CloudFront:\nSecured S3 static website with SSL/TLS Created custom domain with Route 53 Requested SSL certificate from ACM Configured CloudFront distribution Enabled HTTPS for secure connections Implemented in-flight encryption Configured CDN for performance Linked custom domain via DNS Applied SSL certificate to CloudFront Improved website security Enhanced content delivery performance Built production-ready secure website Day 6 - CloudWatch \u0026amp; X-Ray Monitoring:\nImplemented comprehensive monitoring with CloudWatch Set up distributed tracing with X-Ray Monitored Lambda function performance Analyzed application traces Debugged serverless applications Configured CloudWatch Logs Created CloudWatch dashboards Set up CloudWatch Alarms Traced requests across services Identified performance bottlenecks Applied observability best practices Built production monitoring system Overall Week 12 Results:\nCompleted 5 advanced serverless workshops Mastered serverless CI/CD automation Built complete Document Management System Implemented secure authentication with Amplify Configured SSL/TLS for production websites Set up comprehensive monitoring and tracing Ready to deploy enterprise serverless applications "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/3-blogstranslated/3.3-blog3/","title":"Maximizing the value of Smart Machines with generative AI and IoT","tags":[],"description":"","content":"Maximizing the value of Smart Machines with generative AI and IoT by Dimitrios Spiliopoulos, Gabriel Verreault, and Gary Emmerton | on 11 JUL 2025\nIn today\u0026rsquo;s competitive industrial landscape, providers of industrial machines, such as construction and mining equipment, and factory machinery, look for innovative ways to maximize the potential of their products. By connecting these machines to the cloud with IoT, machine builders gain visibility into how their equipment performs in real-world conditions—understanding utilization patterns, identifying recurring failure modes, and discovering optimization opportunities that drive both equipment improvements and new service offerings.\nBuilding a comprehensive machine-to-cloud connected solution can be complex and time-consuming. In the blog Building Smart Industrial Machines with AWS: A Comprehensive Guide, we demonstrated how AWS IoT services enable secure, scalable industrial solutions without extensive infrastructure investment. In this blog, we will explore the transformative potential of combining generative AI with IoT for Smart Machines with a focus on how equipment manufacturers can leverage these capabilities in conjunction to revolutionize industrial operations, create new insights, and drive tangible business outcomes leveraging AWS generative AI services, like Amazon Bedrock, combined with AWS IoT services, like AWS IoT Core and AWS IoT SiteWise.\nGenerative AI for Smart Machines Generative AI can drive significant innovation in industrial use cases by drawing actionable insights from operational and product data. They empower machine operators or service engineers to resolve issues faster, consistently, and repeatably. According to research from Capgemini, a majority of manufacturers are not just curious about generative AI; 55% are actively exploring its potential and 45% are currently working on pilot projects.\nAWS and AWS Partners make it easier for companies to build and scale generative AI-based applications effectively. We will explore four Smart Machines use cases for original equipment manufacturers (OEMs) powered by the combination of generative AI and IoT:\n1. Assisted Diagnosis and Troubleshooting The combination of IoT and generative AI can revolutionize maintenance management. For example, consider the scenario where data from IoT sensors on equipment triggers anomaly alerts due to a threshold, such as temperature, pressure, or vibration, being breached. These alerts can be enriched with additional context from generative AI analysis of equipment manuals, standard operating procedure docs (SOPs), historical maintenance records, captured human experience, spare parts history, and more.\nAccompanying alerts, maintenance teams can receive complete context about the problem, detailed step-by-step repair guidance, specific spare parts recommendations, and a lists of tools needed for the repair. This simplifies troubleshooting and reduces time to repair for both OEM service technicians and end-customer maintenance staff, reducing the need for experienced service engineers on-site.\nFor a sample implementation of this use case, consult Guidance for Assisted Diagnosis and Troubleshooting on AWS. This combination provides comprehensive maintenance guidance that goes beyond fault detection and can even be delivered through voice-enabled AI assistants with speech-to-speech models that allow technicians to verbally request diagnostic information while keeping their hands free for repair work.\n2. Enhanced Field Service Operations Building on the foundation of on-site assisted diagnosis and troubleshooting, machine builders can transform field service operations with generative AI through remote diagnostics and intelligent pre-visit preparation. Field service teams receive AI-generated diagnostic reports, allowing them to analyze machine data, such as sensor readings and error codes, remotely and quickly prepare what is needed for each repair if an on-site visit is needed.\nThis preparation includes spare parts insights, appropriate tools needed, and matching technician expertise to the specific problem. The result is a reduction in multiple site visits, reduced maintenance costs and significant improvements in first-time fix rates and time-to-repair. Better fault analysis and preparation enables better resource allocation, faster repair times, more efficient technician deployment and improved customer experience.\nWatch this video to see how KONE, a global leader in elevators and escalators, together with AWS, developed an AI-powered technician assistant to help technicians fix elevators faster by analyzing maintenance history, IoT data, and relevant documentation from their connected elevators.\n3. Machine Fleet Analysis for OEMs Traditional fleet analytics require data scientists and complex data engineering to extract insights from thousands of deployed machines. Generative AI transforms this by enabling natural language queries across vast datasets, automatically generating narrative insights that combine telemetry data with unstructured sources, like service reports and operator feedback.\nInstead of manually building dashboards to understand performance patterns, OEMs can ask questions, like \u0026ldquo;What are the common failure modes for excavators in high-temperature environments?\u0026rdquo;, and receive comprehensive analyses that identify trends, correlate environmental factors, and recommend design improvements. This makes fleet insights accessible beyond technical teams, enabling sales and product teams to quickly understand how equipment performs across different use cases and environments.\nThese insights can also enable new \u0026lsquo;servitization\u0026rsquo; business models. The system automatically generates detailed performance narratives by asset type, customer segment, and geography, revealing patterns that would previously require significant manual analysis.\n4. AI-generated Diagnostic Reports OEMs can leverage generative AI to create comprehensive operational reports from their products, synthesizing multiple data sources, including telemetry data, maintenance history, environmental conditions, and incident logs. These reports can create strategic insights for their own teams, also saving also time from manual reporting and data collection.\nUnlike reactive diagnostics, these reports analyze broader performance patterns to inform product development, identify design improvements, and optimize service and customer support strategies. These reports can have a dual-reporting approach creating value at both levels: strategic intelligence for manufacturers and automated multidimensional reporting for customers.\nOEMs can offer these diagnostic capabilities as premium services to their customers, generating recurring revenue while building stronger customer relationships through enhanced equipment reliability, better customer service and insights to manage customer operations.\nWatch this video to see how HP is transforming the future of industrial printing with AWS, utilizing IoT, machine learning, and generative AI to create the Intelligent Printing factory of the future.\nBridging IoT Data with Generative AI The Guidance for Deploying Smart Machines on AWS provides a starting point for your Smart Machines journey. This guidance establishes the fundamental building blocks needed to connect and manage smart machines effectively at scale, while creating an industrial data foundation that allows you to prepare, contextualize, and maintain quality data for various applications. With this foundation in place, machine builders can develop new capabilities using generative AI on AWS.\nIndustrial IoT data is ingested either directly into AWS IoT SiteWise using AWS IoT SiteWise Edge or via AWS IoT Core rules. Within the system, this data is organized into digital asset models that combine both static (e.g. serial numbers, machine type) and dynamic properties (e.g. sensor readings, GPS location). These individual assets, such as motors, actuators, and pumps are then connected in parent-child relationships to represent larger machines and equipment.\nThe foundation of structured IoT data becomes even more powerful when combined with generative AI capabilities, transforming raw operational data into actionable insights and intelligent automated responses for assisted maintenance and fleet management analysis. This pattern can also extend to third-party industrial systems such as a maintenance platform to obtain spare parts and maintenance records information.\nAmazon Bedrock: A flexible solution for IoT and generative AI Integration Machine builders working on smart machine solutions can start their generative AI journey with Amazon Bedrock. Amazon Bedrock is a fully managed service that provides easy access to a choice of leading foundation models (FMs). With Amazon Bedrock Knowledge Bases, you can easily connect these models with your organization\u0026rsquo;s data to provide accurate, relevant, and contextual responses based on your specific information sources.\nAmazon Bedrock Knowledge Bases connect with your existing data in Amazon S3, external systems, and customer solutions including Salesforce, Confluence and SharePoint as well as custom endpoints, allowing rapid deployment. Setting up an Amazon Bedrock Knowledge Base is quick and straightforward using the steps defined here.\nCombining the models and your data with Amazon Bedrock Agents allows you to quickly derive insights from your IoT data and proprietary information, regardless of where your data is stored. Through natural language interactions, you can monitor a single machine or seamlessly scale to manage entire fleets and generate operational summaries. Amazon Bedrock Agents can orchestrate and execute tasks against external systems, allowing you to query your IoT data and your maintenance systems to create new incident tickets and alerts.\nFor example, Amazon Bedrock agents can automatically create support tickets in Amazon Connect, enriched with IoT data and knowledge base insights. When equipment issues occur, the system instantly notifies operators with contextual information, enabling faster resolution and proactive customer service. For more details on this contact center pattern, consult Guidance for Connecting Automated Inputs to Contact Centers on AWS.\nAmazon Bedrock also supports multi-agent collaboration to accelerate the build of agentic systems for more complex workflows. See Amazon Bedrock multi-agent collaboration for a hands-on demonstration of this capability.\nEdge Intelligence for Smart Machines Deploying generative AI capabilities directly at the edge is a compelling option for industrial equipment operating in environments with limited or unstable connectivity. Depending on connectivity and compute availability, a hybrid approach can also be employed where smaller routine tasks execute against the edge models and more complex queries are offloaded to large language models (LLMs) in the cloud if connectivity is available.\nEmbedding small language models (SLMs) directly into their smart machines is also possible, enabling continuous AI-powered support even in offline conditions. Check out these blogs for a deeper dive on Edge Intelligence with AWS and Best Practices for Generative AI and IoT at the edge.\nAWS IoT SiteWise Assistant: A native solution for Industrial IoT and generative AI integration While Amazon Bedrock provides a flexible foundation for any IoT data source, AWS also offers a purpose-built solution for industrial equipment manufacturers who utilize AWS IoT SiteWise for their industrial data collection, storage, and analysis needs. AWS IoT SiteWise Assistant enhances AWS IoT SiteWise capabilities by enabling natural language queries of your machine data instead of needing to write complex technical query statements to gain insights into machine performance, trends, and operational metrics.\nFor a more detailed overview, see Transforming industrial decision making with AWS IoT SiteWise Assistant.\nGenerative AI within connected Smart Machines with AWS Partners AWS Partners play a key role in supporting AWS customers building smart machine solutions, from strategy consulting and ideation to delivery of scalable and secure solutions that leverage the Guidance for Deploying Smart Machines on AWS. In this section, we address how some of these system integrator (SI) AWS Partners are helping customers and advancing the broader smart machine industry with the combination of IoT and generative AI.\nDeloitte harnesses generative AI to enable assisted maintenance and autonomous decision-making for Smart Machines, optimizing both aftermarket support and the shift towards Equipment-as-a-Service (EaaS).\nSoftServe\u0026rsquo;s unified IIoT platform and generative AI solution combine Industrial IoT (IIoT), generative AI, Digital Twins and NVIDIA capabilities to transform Smart Machines. Their Schunk implementation empowers remote and field support engineers to efficiently troubleshoot customer equipment.\nTwisthink is now supporting Smart Machines use cases using generative AI for prescriptive maintenance and fleet management analytics with less development effort and investment.\nGreen Custard combines IoT and generative AI on AWS to revolutionize its Smart Machine offerings through assisted maintenance real-time performance optimization, and enhanced customer support. Their Britvic\u0026rsquo;s Aqua Libra Flavour Tap implementation demonstrates enhanced customer service through intelligent analysis of IoT data and support documentation.\nConclusion Combining IoT and generative AI is transformative for smart machine builders, enhancing maintenance operations, expanding fleet management processes, and generating new revenue streams through new applications for your customers. AWS and AWS Partners offer services and solutions to help manufacturers expand their current IoT solutions or build new smart machines that positions manufacturers to stay competitive and drive revenue growth.\nReady to transform your products with IoT and generative AI? Start your smart machine journey today by reaching out to AWS or your AWS partner. For more information on generative AI at AWS, see the resources below:\nGenerative AI for Industrial Generative AI Innovation Center Best practices for building robust generative AI applications with Amazon Bedrock Agents AWS Internet of Things Tags: AWS IoT, aws manufacturing, Generative AI, Smart Machines\nAuthors:\nDimitrios Spiliopoulos - Worldwide Principal IIoT GTM Specialist in AWS Gabriel Verreault - Senior Partner Solutions Architect at AWS for the Manufacturing segment Gary Emmerton - Senior Solutions Architect in AWS based in the UK Source: AWS for Industries Blog\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Announcing the Developer Preview of DynamoDB Mapper for Kotlin This blog announces the Developer Preview of DynamoDB Mapper for Kotlin, a high-level library that provides streamlined, idiomatic ways for developers to map data between their Kotlin business logic and DynamoDB tables. You will learn how to use the DynamoDB Mapper Schema Generator plugin for Gradle with annotated data classes, understand the preview functionality including supported operations (deleteItem, getItem, putItem, queryPaginated, scanPaginated) and expression types, and get started with code examples showing how to automatically generate item schemas and perform CRUD operations on DynamoDB tables using native Kotlin objects.\nBlog 2 - Announcing General Availability of the AWS SDK for Swift This blog announces the General Availability of the AWS SDK for Swift, AWS\u0026rsquo;s solution for accessing Amazon Web Services from Swift applications. You will discover how to integrate AWS services into native Apple platform apps, AWS Lambda, and Swift on Server applications with full support for Swift Structured Concurrency (async/await), binary data streaming, event streaming, waiters, paginators, and automatic retry. The article provides a quick start guide with complete code examples, explains the flexible HTTP client options (CRT-based and URLSession-based), and shows how AWS SDK for Swift works across multiple platforms including macOS, iOS, iPadOS, tvOS, watchOS, visionOS, and Linux.\nBlog 3 - Maximizing the value of Smart Machines with generative AI and IoT This blog explores how combining generative AI with IoT transforms smart machines for industrial equipment manufacturers. You will learn about four key use cases: assisted diagnosis and troubleshooting that enriches IoT sensor alerts with AI-generated repair guidance, enhanced field service operations through remote diagnostics and intelligent pre-visit preparation, machine fleet analysis using natural language queries across vast datasets, and AI-generated diagnostic reports for strategic insights. The article demonstrates how to bridge IoT data with generative AI using Amazon Bedrock, AWS IoT SiteWise Assistant, and edge intelligence, along with real-world implementations from AWS partners including KONE\u0026rsquo;s AI-powered technician assistant and HP\u0026rsquo;s intelligent printing factory.\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event\u0026rsquo;s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event.\nDuring my internship, I had the opportunity to participate in three technology events organized by AWS, focusing on Generative AI and modern software development. Each event was a memorable experience that provided new, interesting, and highly valuable knowledge about applying AI in practice, along with meaningful connections and wonderful moments.\nEvent 1 Event Name: AWS Cloud Day Vietnam 2025 – AI Edition\nTheme: New Age Vietnam: From Vision to Value\nDate: September 18, 2025\nLocation: Vietnam\nRole: Attendee\nKey Content: The event focused on transforming AI vision into measurable value, covering topics on Generative AI, reference architectures (RAG, agentic workflows), data security, and deployment roadmap from PoC to Scale on AWS platform.\nEvent 2 Event Name: AI-powered planning, design, and coding for modern software development\nFormat: AWS Marketplace on-demand webinar\nDate: October 1, 2025\nRole: Attendee\nKey Content: Webinar demonstrating how to integrate Generative AI into the SDLC (planning, design, coding), including sprint planning, test generation, automated documentation/architecture generation, and applying zero-trust security for code analysis and architecture reviews.\nEvent 3 Event Name: AI-Driven Development Lifecycle: Reshaping Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nKey Content: Workshop introducing the AIDOC (AI Development Lifecycle) methodology, how to use Amazon Q Developer and Kiro to boost software development productivity, with emphasis on human-AI collaboration throughout the entire development lifecycle.\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.4-setup_lambda/","title":"Setup Lambda","tags":[],"description":"","content":"1. Go to Lambda You can code with Lambda using languages as shown in the image. In this workshop, we will use Node.js 2. Click on Create function Fill in the function name and select the runtime type Click on \u0026ldquo;Change default execution role\u0026rdquo;, select \u0026ldquo;existing role\u0026rdquo;, then click on the role you created earlier for the workshop Click on \u0026ldquo;Create function\u0026rdquo; 3. Set up function\nGo to the Configuration tab Increase memory to 500MB and timeout to 2 minutes Go to the Code tab and paste this code: Click the Deploy button const { BedrockRuntimeClient, InvokeModelCommand } = require(\u0026#34;@aws-sdk/client-bedrock-runtime\u0026#34;); const client = new BedrockRuntimeClient({ region: \u0026#34;us-east-1\u0026#34; }); const CORS_HEADERS = { \u0026#34;Access-Control-Allow-Origin\u0026#34;: process.env.ALLOWED_ORIGIN || \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;OPTIONS,POST\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }; const MAX_MESSAGE_LENGTH = 2000; const MAX_HISTORY_TURNS = 10; exports.handler = async (event) =\u0026gt; { const startTime = Date.now(); console.log(\u0026#34;Request received:\u0026#34;, { sourceIp: event.requestContext?.identity?.sourceIp || event.requestContext?.http?.sourceIp, userAgent: event.requestContext?.identity?.userAgent || event.headers?.[\u0026#34;user-agent\u0026#34;] }); const httpMethod = event.httpMethod || event.requestContext?.http?.method; if (httpMethod === \u0026#34;OPTIONS\u0026#34;) { return { statusCode: 200, headers: CORS_HEADERS, body: \u0026#34;\u0026#34; }; } let body; try { body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); } catch (e) { console.error(\u0026#34;Invalid JSON:\u0026#34;, e.message); return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Invalid JSON format\u0026#34; }) }; } const userMessage = (body.message || \u0026#34;\u0026#34;).toString().trim(); const history = Array.isArray(body.history) ? body.history : []; if (!userMessage) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;Message is required\u0026#34; }) }; } if (userMessage.length \u0026gt; MAX_MESSAGE_LENGTH) { return { statusCode: 400, headers: CORS_HEADERS, body: JSON.stringify({ error: `Message too long. Maximum ${MAX_MESSAGE_LENGTH} characters allowed.` }) }; } // Claude uses messages format const messages = []; const recentHistory = history.slice(-MAX_HISTORY_TURNS); for (const turn of recentHistory) { if (turn.user) { messages.push({ role: \u0026#34;user\u0026#34;, content: turn.user }); } if (turn.assistant) { messages.push({ role: \u0026#34;assistant\u0026#34;, content: turn.assistant }); } } messages.push({ role: \u0026#34;user\u0026#34;, content: userMessage }); const requestBody = { anthropic_version: \u0026#34;bedrock-2023-05-31\u0026#34;, max_tokens: 512, messages: messages, temperature: 0.7 }; const modelId = \u0026#34;arn:aws:bedrock:us-east-1:756859458422:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0\u0026#34;; try { console.log(\u0026#34;Invoking Bedrock model:\u0026#34;, modelId); const command = new InvokeModelCommand({ modelId, contentType: \u0026#34;application/json\u0026#34;, accept: \u0026#34;application/json\u0026#34;, body: JSON.stringify(requestBody) }); const response = await client.send(command); const result = JSON.parse(new TextDecoder().decode(response.body)); const reply = result.content[0].text || \u0026#34;Sorry, I cannot generate a response.\u0026#34;; const duration = Date.now() - startTime; console.log(\u0026#34;Request completed:\u0026#34;, { duration, responseLength: reply.length }); return { statusCode: 200, headers: CORS_HEADERS, body: JSON.stringify({ response: reply }) }; } catch (error) { console.error(\u0026#34;Bedrock invocation error:\u0026#34;, { message: error.message, code: error.code, modelId: modelId }); return { statusCode: 500, headers: CORS_HEADERS, body: JSON.stringify({ error: \u0026#34;AI service is temporarily unavailable. Please try again later.\u0026#34; }) }; } }; ``` **Related Documentation:** https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.5-setup-api-gateway/","title":"Setup API Gateway","tags":[],"description":"","content":"1. Go to API Gateway\nClick \u0026ldquo;Create an API\u0026rdquo; Select \u0026ldquo;Build\u0026rdquo; under REST API Enter a name Choose a security policy Click \u0026ldquo;Create API\u0026rdquo; 2. Create Resource\nClick \u0026ldquo;Create Resource\u0026rdquo; Name the resource and enable CORS Click the \u0026ldquo;Create Resource\u0026rdquo; button 3. Create Method\nClick \u0026ldquo;Create Method\u0026rdquo; Select POST and enable Lambda Proxy Integration Add the Lambda function ARN Click \u0026ldquo;Create Method\u0026rdquo; 4. Deploy API\nClick \u0026ldquo;Deploy API\u0026rdquo; Choose \u0026ldquo;New Stage\u0026rdquo; and name the stage Click the \u0026ldquo;Deploy\u0026rdquo; button 5. Test\nAfter creating the stage, you will get the Invoke URL Use the Invoke URL to test in Postman with the format InvokeURL/, using the POST method Use the following JSON body:\n{ \u0026ldquo;message\u0026rdquo;: \u0026ldquo;What is Amazon Bedrock? Please answer briefly, no more than 3 lines.\u0026rdquo; }\nClick \u0026ldquo;Send\u0026rdquo; You should receive a successful response\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Overview Amazon Bedrock provides the ability to integrate leading foundation models (LLMs) from Anthropic, Meta, AI21 Labs, and other providers through a simple API, allowing you to build AI applications without managing complex infrastructure.\nIn this workshop, we will learn how to build, deploy, and test a complete AI Chatbot using a serverless architecture, enabling users to interact with Claude Haiku 4.5 without having to manage servers or worry about scaling.\nWe will create a system with five main components to build the chatbot: Amazon Bedrock (AI engine), AWS Lambda (backend logic), API Gateway (REST API endpoint), Amazon S3 (frontend hosting), and CloudWatch (monitoring). These components deliver a fully serverless architecture with low cost and automatic scaling.\nMain Components: Amazon Bedrock (AI Engine) – Provides the Claude Haiku 4.5 model through a simple API. You call the InvokeModel API to send messages and receive intelligent AI responses.\nAWS Lambda (Backend Logic) – Runs Node.js 24 code to process requests from API Gateway. Lambda validates input (message length, history limits), formats prompts for Bedrock, calls the Bedrock API, and handles errors.\nAPI Gateway (REST API Endpoint) – Creates a public HTTPS endpoint (POST /chat) for the frontend to call. API Gateway handles CORS configuration, routes requests to Lambda, and provides throttling to protect the backend from abuse.\nAmazon S3 (Frontend Hosting) – Hosts a static website (HTML) for the chatbot UI. Users access the chatbot through the browser; the interface sends messages to API Gateway and displays AI responses.\nCloudWatch (Monitoring \u0026amp; Debugging) – Automatically collects logs from Lambda execution, allowing you to debug errors.\nArchitecture Overview User Browser → S3 (Static Website) → API Gateway → Lambda → Bedrock (Claude) → CloudWatch\nFlow:\nUser enters a message in the chatbot UI (hosted on S3) JavaScript sends a POST request to the API Gateway endpoint API Gateway invokes the Lambda function Lambda validates input, formats the prompt, and calls the Bedrock InvokeModel API Bedrock processes it with Claude Haiku 4.5 and returns the AI response Lambda returns the response back to API Gateway → Browser CloudWatch logs the entire process Workshop Modules Module 1: Setup Amazon Bedrock\n• Enable Claude Haiku 4.5 model access\n• Understand inference profiles\n• Test the model via AWS Console\nModule 2: Create Lambda Function\n• Create a Node.js 24 Lambda function\n• Deploy chatbot backend code\n• Configure IAM role with Bedrock permissions\n• Set environment variables\nModule 3: Configure API Gateway\n• Create REST API\n• Configure POST /chat endpoint\n• Enable CORS\n• Test API with Postman\nModule 4: Deploy Frontend to S3\n• Create S3 bucket\n• Enable static website hosting\n• Upload HTML chatbot UI\n• Configure public access\nModule 5: Testing \u0026amp; Debugging\n• Test end-to-end flow\n• View CloudWatch logs\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.6-monitoring-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":"1. In Lambda, go to the Monitor tab\nClick \u0026ldquo;View CloudWatch logs\u0026rdquo; You can view the function logs via Log Streams "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/5-workshop/5.7-setup-s3/","title":"Prepare S3","tags":[],"description":"","content":"1. Create an HTML file to deploy to S3\nReplace InvokeURL/resourceName in the code below with your invoke URL. \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Chatbot - ChatGPT Style\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; font-family: \u0026#34;Inter\u0026#34;, \u0026#34;Segoe UI\u0026#34;, sans-serif; background-color: #f0f0f0; display: flex; flex-direction: column; height: 100vh; } header { background-color: #2e7d32; /* Green */ color: white; padding: 1rem; text-align: center; font-weight: bold; font-size: 1.2rem; } #chatBox { flex: 1; overflow-y: auto; padding: 1.5rem; display: flex; flex-direction: column; gap: 1rem; } /* ChatGPT-like message blocks */ .message { width: 100%; max-width: 800px; margin: auto; padding: 1rem; border-radius: 10px; font-size: 1rem; line-height: 1.5; border: 1px solid #ddd; background-color: #ffffff; } .user { background-color: #e8f5e9; border-color: #c8e6c9; } .bot { background-color: #ffffff; border-color: #e0e0e0; } footer { padding: 1rem; background-color: #f7f7f8; border-top: 1px solid #ddd; display: flex; justify-content: center; } .input-container { width: 100%; max-width: 800px; display: flex; gap: 0.5rem; background: white; border: 1px solid #ccc; padding: 0.75rem; border-radius: 12px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); } input { flex: 1; border: none; outline: none; font-size: 1rem; background: none; } button { background-color: #4caf50; color: white; border: none; padding: 0.6rem 1rem; font-size: 1rem; border-radius: 8px; cursor: pointer; transition: 0.2s ease; } button:hover { background-color: #449d48; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt;🌿 Chatbot\u0026lt;/header\u0026gt; \u0026lt;div id=\u0026#34;chatBox\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;input-container\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;userInput\u0026#34; placeholder=\u0026#34;Send a message...\u0026#34; /\u0026gt; \u0026lt;button onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;script\u0026gt; let history = []; async function sendMessage() { const userInput = document.getElementById(\u0026#34;userInput\u0026#34;); const message = userInput.value.trim(); if (!message) return; addMessage(message, \u0026#34;user\u0026#34;); const response = await fetch(\u0026#34;InvokeURL/resourceName\u0026#34;, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ message, history }), }); const data = await response.json(); const botReply = data.response; addMessage(botReply, \u0026#34;bot\u0026#34;); history.push({ user: message, assistant: botReply }); userInput.value = \u0026#34;\u0026#34;; } function addMessage(text, sender) { const msg = document.createElement(\u0026#34;div\u0026#34;); msg.classList.add(\u0026#34;message\u0026#34;, sender); msg.textContent = text; document.getElementById(\u0026#34;chatBox\u0026#34;).appendChild(msg); msg.scrollIntoView({ behavior: \u0026#34;smooth\u0026#34; }); } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2. Go to S3\nClick Create bucket\nEnter a bucket name\nDisable Block public access\n(While the best practice is keeping the bucket private, for this workshop we set it to public for easier testing.)\nClick Create bucket\n3. Open the bucket you just created\nGo to the Properties tab\nClick Edit under Static website hosting\nSelect Enable\nClick Save changes\nGo to the Permissions tab → Edit Bucket policy\nPaste the following policy (replace your-bucket-name with your actual bucket name):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Click Save changes\nGo to the Objects tab and upload your HTML file\nClick the Object URL to open your deployed HTML page\nDeployment completed successfully\n"},{"uri":"https://khanhtm45.github.io/AWS-FCJ/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment at FCJ is very friendly, open, and professional. Everyone is always enthusiastic to help when I need assistance, even after working hours. The workspace is well-organized and creates a comfortable atmosphere that helps me maintain focus. Perhaps it would be even better if there were more social activities or team building events to enhance bonding among members.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe assigned tasks are quite relevant to my major, while also helping me explore new fields that I hadn\u0026rsquo;t had the chance to learn about before. This helps me both review foundational knowledge and acquire additional practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship period, I\u0026rsquo;ve learned many new skills such as how to use project management tools, teamwork abilities, and professional communication in a business environment. The mentor also regularly shares valuable real-world experiences, helping me gain a clearer perspective on my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe work culture here is very positive and dynamic: everyone respects each other, works seriously but doesn\u0026rsquo;t lack enjoyment. When there are tight deadlines, the whole team works together and supports each other regardless of position. This makes me feel accepted as a real member, even though I\u0026rsquo;m just an intern.\n6. Internship Policies / Benefits\nThe company has a support allowance system for interns and allows flexible working hours when there\u0026rsquo;s a need. Especially, being able to attend internal training sessions is a very big advantage.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://khanhtm45.github.io/AWS-FCJ/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khanhtm45.github.io/AWS-FCJ/tags/","title":"Tags","tags":[],"description":"","content":""}]